#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/isaac/modular_isaac.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_isaac.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 Perceptron, Inc and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import copy
import itertools
import math
from collections.abc import Callable, Iterable
from dataclasses import dataclass, field, replace
from enum import IntEnum
from typing import Any, Optional, Union

from ...activations import ACT2FN
from ...cache_utils import Cache, DynamicCache
from ...configuration_utils import PretrainedConfig
from ...generation.utils import GenerationMixin
from ...image_processing_utils_fast import ImagesKwargs
from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func
from ...masking_utils import ALL_MASK_ATTENTION_FUNCTIONS, create_masks_for_generate, packed_sequence_mask_function
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...models.auto.modeling_auto import AutoModel
from ...models.qwen3.modeling_qwen3 import Qwen3PreTrainedModel
from ...processing_utils import Unpack
from ...utils import auto_docstring
from ...utils.generic import OutputRecorder, TransformersKwargs, can_return_tuple, check_model_inputs
from ...utils.import_utils import (
    is_torch_available,
    is_torchdynamo_compiling,
)
from ..qwen2_5_vl import modeling_qwen2_5_vl as qwen2_5_vl_modeling
from .configuration_isaac import IsaacConfig, IsaacVisionConfig


if is_torch_available():
    import torch
    import torch.nn as nn
    import torch.nn.functional as F


# TENSORSTREAM START -------------------------------------------------------------------------------


class ModalityType(IntEnum):
    """
    Modality identifiers for events.

    Members:
        image: Vision tokens (e.g., patches).
        text: Textual tokens.
        padding: Padding tokens used in sequence batching.
    """

    image = 0
    text = 1
    padding = 2


# @dataclass
@dataclass(slots=True)
class Event:
    """
    Represents a single data occurrence (with a specific type, time interval, and data payload).

    Attributes:
        data (Any): The actual data payload (e.g. a torch.Tensor, a string, etc.).
        type (ModalityType): The modality type of the data (e.g., ModalityType.image).
        time (Tuple[float, float]): (start_time, end_time) indicating when this Event occurs.
        role (Optional[str]): The role associated with this event (e.g., "user", "agent", "system").
            If None, the event is always included in loss calculation.

    Example usage:
        evt = Event(data=torch.zeros((1, 224, 224, 3)),  # e.g. a single image frame
                    type=ModalityType.image,
                    time=(0.0, 0.04),
                    role="user")

    """

    # Descriptors
    data: Any
    time: tuple[float, float]
    type: ModalityType
    role: str | None = None

    # Structure
    dims_virtual: list[int] | None = None  # virtual/processed dimensions (e.g., pixel-shuffled)
    dims_real: list[int] | None = None  # real/actual tensor dimensions
    idx_range: tuple[int, int] | None = None

    # Misc Tags (data source, shard idx, etc.)
    tags: dict = field(default_factory=dict)

    def dims(self, virtual: bool = True) -> list[int] | None:
        """
        Get the dimensions of this event.

        Args:
            virtual: If True (default), return virtual/processed dimensions (e.g., pixel-shuffled).
                    If False, return real/actual tensor dimensions.

        Returns:
            Dimensions list or None if not measured.
        """
        if virtual:
            return self.dims_virtual
        else:
            return self.dims_real

    @property
    def is_measured(self):
        return self.dims_virtual is not None

    def num_tokens(self, partial=True, virtual=True) -> int:
        if not virtual:
            assert partial is False and isinstance(self.data, torch.Tensor)
            return math.prod(self.dims(virtual=False))
        return self.idx_range[1] - self.idx_range[0] if partial else math.prod(self.dims())

    def shallow_copy(self) -> "Event":
        return replace(self)


@dataclass
class Stream:
    """
    Represents an ordered sequence of Event objects, each with
    a specific ModalityType and a time range.

    Attributes:
        events (List[Event]): The list of Event objects in the stream.
        priority (List[ModalityType]): A list of modality types that define
            how we might want to reorder or prioritize events if scheduling is needed.

    Example usage:
        # Create two events of different types
        evt1 = Event(torch.zeros((1, 224, 224, 3)), ModalityType.image, (0.0, 0.04))
        evt2 = Event(torch.randint(0, 1000, (16, 1)), ModalityType.text, (0.0, 0.32))

        # Make a stream with a given priority
        s = Stream(events=[evt1, evt2],
                   priority=[ModalityType.image, ModalityType.text])

        print(s)
    """

    events: list[Event]
    priority: list[ModalityType]  # priority of stream ordering

    def __len__(self):
        """Returns the number of Event objects in this Stream."""
        return len(self.events)

    def __getitem__(self, key: int) -> "Stream | Event":
        return self.events[key]

    def __iter__(self):
        """
        Yields each Event in the Stream, enabling iteration like:
            for event in my_stream:
                ...
        """
        yield from self.events

    def shallow_copy(self) -> "Stream":
        events_copy = [ev.shallow_copy() for ev in self.events]
        return Stream(events=events_copy, priority=self.priority)


# TODO: implement all types of cool indexing which can happen since TensorStream assuems Event.data = Tensor
@dataclass
class TensorStream:
    streams: list[Stream]
    _device: torch.device | None = None

    def __post_init__(self):
        for stream in self.streams:
            for event in stream.events:
                assert isinstance(event.data, torch.Tensor)
                if self._device is None:
                    self._device = torch.device(event.data.device)

    @property
    def device(self):
        return self._device

    @property
    def shape(self):
        seq_lens = [sum([ev.num_tokens() for ev in stream]) for stream in self.streams]
        assert all([sl == seq_lens[0] for sl in seq_lens]), (
            f"each stream must have same token count to have a shape: {seq_lens}"
        )
        return (len(seq_lens), seq_lens[0])

    def to(
        self,
        device: torch.device | str,
        dtype: torch.dtype | None = None,
        non_blocking: bool = True,
    ) -> "TensorStream":
        """
        Move **all** `Event.data` tensors to *device*.

        We send each tensor individually instead of the
        flatten â†’ unflatten round-trip:

        * one async H2D copy per tensor (still overlapped when
          `pin_memory=True` is set on the DataLoader),
        * no extra host-side concat, no extra device allocation,
        * `requires_grad` flags are preserved.

        NOTE: textual modalities are always cast to `torch.long`;
        everything else keeps its original
        dtype unless an explicit *dtype* argument is supplied.
        """
        target_device = torch.device(device)

        for stream in self.streams:
            for ev in stream:
                # ------------------------------------------------------------------
                # Decide the dtype for *this* event.
                # ------------------------------------------------------------------
                if ev.type in {ModalityType.text, ModalityType.padding}:
                    tgt_dtype = torch.long
                else:
                    tgt_dtype = dtype or ev.data.dtype

                # ------------------------------------------------------------------
                # Perform the device / dtype move.
                # ------------------------------------------------------------------
                # We clone no tensor here; torch will reuse storage
                # if `dtype` and `device` are unchanged.
                moved = ev.data.to(
                    device=target_device,
                    dtype=tgt_dtype,
                    non_blocking=non_blocking,
                )

                # Preserve autograd leaf & grad-enabled state.
                moved.requires_grad_(ev.data.requires_grad)

                ev.data = moved

        # Remember where the whole TensorStream lives now.
        self._device = target_device
        return self


class IsaacImageProcessorFastKwargs(ImagesKwargs, total=False):
    patch_size: Optional[int]
    max_num_patches: Optional[int]
    min_num_patches: Optional[int]
    pixel_shuffle_scale: Optional[int]


class IsaacVisionEmbeddings(nn.Module):
    """Adapter around SigLIP2 vision embeddings that consumes packed patch sequences.

    Isaac accepts variable-resolution vision inputs as a single packed sequence with per-image
    `token_grids`; packing/unpacking here reconstructs per-image shapes so we can resize positional
    embeddings and build `cu_seqlens` for variable-length attention (not generic generation packing).
    """

    def __init__(self, config: IsaacVisionConfig):
        super().__init__()
        self.config = config
        self.embed_dim = config.hidden_size
        self.patch_size = config.patch_size

        self.patch_embedding = nn.Linear(
            in_features=config.num_channels * self.patch_size * self.patch_size,
            out_features=self.embed_dim,
        )

        self.num_patches = config.num_patches
        self.position_embedding_size = int(self.num_patches**0.5)
        self.position_embedding = nn.Embedding(self.num_patches, self.embed_dim)

    def forward(self, seq_patches: torch.Tensor, spatial_shapes: torch.Tensor) -> torch.Tensor:
        # Rebatch packed variable-resolution patches to resize per-image position embeddings
        # and track lengths for varlen attention metadata.
        packed_pixel_values, seq_lengths = self._pack_to_batch(seq_patches, spatial_shapes)
        if packed_pixel_values is None:
            return seq_patches.new_zeros((0, self.embed_dim))

        target_dtype = self.patch_embedding.weight.dtype
        patch_embeds = self.patch_embedding(packed_pixel_values.to(dtype=target_dtype))

        positional_embeddings = self.position_embedding.weight.reshape(
            self.position_embedding_size,
            self.position_embedding_size,
            -1,
        )
        resized_positional_embeddings = self.resize_positional_embeddings(
            positional_embeddings, spatial_shapes, max_length=packed_pixel_values.shape[1]
        )

        embeddings = patch_embeds + resized_positional_embeddings
        return self._unpack_from_batch(embeddings, seq_lengths)

    @staticmethod
    def resize_positional_embeddings(
        positional_embeddings: torch.Tensor,
        spatial_shapes: torch.LongTensor,
        max_length: int,
    ) -> torch.Tensor:
        """
        Resize positional embeddings to image-specific size and pad to a fixed size.

        Args:
            positional_embeddings (`torch.Tensor`):
                Position embeddings of shape (height, width, embed_dim)
            spatial_shapes (`torch.LongTensor`):
                Spatial shapes of shape (batch_size, 2) to resize the positional embeddings to
            max_length (`int`):
                Maximum length of the positional embeddings to pad resized positional embeddings to

        Returns:
            `torch.Tensor`: Embeddings of shape (batch_size, max_length, embed_dim)
        """
        batch_size = spatial_shapes.shape[0]
        embed_dim = positional_embeddings.shape[-1]
        source_dtype = positional_embeddings.dtype

        resulted_positional_embeddings = torch.empty(
            (batch_size, max_length, embed_dim),
            device=positional_embeddings.device,
            dtype=source_dtype,
        )

        # (height, width, embed_dim) -> (1, embed_dim, height, width) for interpolation
        positional_embeddings = positional_embeddings.permute(2, 0, 1).unsqueeze(0)

        # Upcast to float32 on CPU because antialias is not supported for bfloat16/float16 on CPU
        if positional_embeddings.device.type == "cpu":
            positional_embeddings = positional_embeddings.to(torch.float32)

        for i in range(batch_size):
            # (1, dim, height, width) -> (1, dim, target_height, target_width)
            height, width = spatial_shapes[i]
            resized_embeddings = F.interpolate(
                positional_embeddings,
                size=(height, width),
                mode="bilinear",
                align_corners=False,
                antialias=True,
            )

            # (1, dim, target_height, target_width) -> (target_height * target_width, dim)
            resized_embeddings = resized_embeddings.reshape(embed_dim, height * width).transpose(0, 1)

            # Cast to original dtype
            resized_embeddings = resized_embeddings.to(source_dtype)

            resulted_positional_embeddings[i, : height * width] = resized_embeddings
            resulted_positional_embeddings[i, height * width :] = resized_embeddings[0]

        return resulted_positional_embeddings

    def _pack_to_batch(
        self,
        seq_patches: torch.Tensor,
        spatial_shapes: torch.Tensor,
    ) -> tuple[Optional[torch.Tensor], torch.Tensor]:
        """Rebatch a packed patch sequence using per-image grids to align embeddings.

        Args:
            seq_patches (`torch.Tensor`): Packed patches of shape `(total_patches, patch_dim)`.
            spatial_shapes (`torch.Tensor`): Per-image patch grids of shape `(num_images, 2)` as `(H_tokens, W_tokens)`.

        Returns:
            `tuple[Optional[torch.Tensor], torch.Tensor]`: A padded batch tensor shaped
            `(batch, max_len, patch_dim)` plus `seq_lengths` used to form `cu_seqlens` for
            variable-length attention.
        """
        if seq_patches.ndim != 2:
            raise ValueError("`seq_patches` is expected to be 2D (total_patches, patch_dim).")
        if spatial_shapes.ndim != 2 or spatial_shapes.size(-1) != 2:
            raise ValueError("`spatial_shapes` must have shape (num_images, 2) with (height_tokens, width_tokens).")

        seq_lengths = spatial_shapes.long().prod(dim=-1)
        total_patches = int(seq_lengths.sum().item())
        if total_patches != seq_patches.size(0):
            raise ValueError(
                "Mismatch between packed patches and spatial shapes: got "
                f"{seq_patches.size(0)} patches but spatial shapes imply {total_patches}."
            )

        batch_size = spatial_shapes.size(0)
        if batch_size == 0:
            return None, seq_lengths

        max_length = int(seq_lengths.max().item())
        patch_dim = seq_patches.size(-1)
        device = seq_patches.device

        packed_pixel_values = seq_patches.new_zeros((batch_size, max_length, patch_dim), device=device)

        start = 0
        for batch_idx, length in enumerate(seq_lengths.tolist()):
            if length == 0:
                continue
            end = start + length
            packed_pixel_values[batch_idx, :length] = seq_patches[start:end]
            start = end

        return packed_pixel_values, seq_lengths

    def _unpack_from_batch(self, embeddings: torch.Tensor, seq_lengths: torch.Tensor) -> torch.Tensor:
        """Flatten a padded batch back to packed sequence order using `seq_lengths`."""
        output_chunks: list[torch.Tensor] = []
        for batch_idx, length in enumerate(seq_lengths.tolist()):
            if length == 0:
                continue
            output_chunks.append(embeddings[batch_idx, :length])

        if not output_chunks:
            return embeddings.new_zeros((0, embeddings.size(-1)))

        return torch.cat(output_chunks, dim=0)


class IsaacVisionAttention(nn.Module):
    """Custom attention that supports variable-length sequences with flash attention."""

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.embed_dim = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.embed_dim // self.num_heads
        if self.head_dim * self.num_heads != self.embed_dim:
            raise ValueError(
                f"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:"
                f" {self.num_heads})."
            )
        self.scale = self.head_dim**-0.5
        self.dropout = config.attention_dropout
        self.is_causal = False

        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        output_attentions: bool = False,
        cu_seqlens: Optional[torch.Tensor] = None,
        max_seqlen: Optional[int] = None,
        **kwargs,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Input shape: Batch x Time x Channel"""
        kwargs.pop("output_hidden_states", None)
        kwargs.pop("return_dict", None)

        batch_size, seq_length, embed_dim = hidden_states.shape
        queries = self.q_proj(hidden_states)
        keys = self.k_proj(hidden_states)
        values = self.v_proj(hidden_states)

        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)

        attn_impl = self.config._attn_implementation
        attention_interface: Callable = ALL_ATTENTION_FUNCTIONS["sdpa"]
        if attn_impl != "sdpa":
            attention_interface = ALL_ATTENTION_FUNCTIONS[attn_impl]

        dropout = 0.0 if not self.training else self.dropout
        attention_kwargs: dict[str, Any] = {
            "is_causal": False,
            "scaling": self.scale,
            "dropout": dropout,
        }

        supports_varlen = cu_seqlens is not None and attn_impl in {
            "flash_attention_2",
            "flash_attention_3",
            "flex_attention",
            "paged|flash_attention_2",
            "paged|flash_attention_3",
        }

        if output_attentions and attn_impl == "eager":
            attention_kwargs["output_attentions"] = True

        if supports_varlen:
            if max_seqlen is not None:
                max_q = max_k = int(max_seqlen)
            elif cu_seqlens.numel() >= 2:
                lengths = cu_seqlens[1:] - cu_seqlens[:-1]
                max_q = max_k = lengths.max() if lengths.numel() > 0 else seq_length
            else:
                max_q = max_k = seq_length

            attention_kwargs.update(
                {
                    "cu_seq_lens_q": cu_seqlens,
                    "cu_seq_lens_k": cu_seqlens,
                    "max_length_q": max_q,
                    "max_length_k": max_k,
                }
            )

        attn_output, attn_weights = attention_interface(
            self,
            queries,
            keys,
            values,
            attention_mask,
            **attention_kwargs,
        )

        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()

        attn_output = self.out_proj(attn_output)

        return attn_output, attn_weights


class IsaacMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.activation_fn = ACT2FN[config.hidden_act]
        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        hidden_states = self.fc1(hidden_states)
        hidden_states = self.activation_fn(hidden_states)
        hidden_states = self.fc2(hidden_states)
        return hidden_states


class IsaacVisionEncoderLayer(GradientCheckpointingLayer):
    """Isaac vision encoder layer with variable-length attention."""

    def __init__(self, config: IsaacVisionConfig):
        super().__init__()
        self.embed_dim = config.hidden_size
        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)
        self.self_attn = IsaacVisionAttention(config)
        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)
        self.mlp = IsaacMLP(config)

    @auto_docstring
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        cu_seqlens: Optional[torch.Tensor] = None,
        max_seqlen: Optional[int] = None,
        output_attentions: bool = False,
        **kwargs: Unpack[TransformersKwargs],
    ) -> torch.FloatTensor:
        r"""
        cu_seqlens (`torch.Tensor`, *optional*):
            Prefix-sum tensor whose length equals the number of documents + 1. The difference between successive
            entries gives each document's token count and enables block-diagonal attention masking for packed batches.
        max_seqlen (`int`, *optional*):
            Maximum document length referenced by `cu_seqlens`. Passed to FlashAttention so it can size temporary
            buffers for packed variable-length attention.
        """
        # Run attention directly so variable-length metadata reaches FlashAttention.
        residual = hidden_states
        hidden_states = self.layer_norm1(hidden_states)
        attn_output, _ = self.self_attn(
            hidden_states,
            attention_mask=attention_mask,
            cu_seqlens=cu_seqlens,
            max_seqlen=max_seqlen,
            **kwargs,
        )
        hidden_states = residual + attn_output

        residual = hidden_states
        hidden_states = self.layer_norm2(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states


class IsaacVisionEncoder(nn.Module):
    """Encoder using Isaac encoder layers with variable-length attention support."""

    def __init__(self, config: IsaacVisionConfig):
        super().__init__()
        self.config = config
        self.layers = nn.ModuleList([IsaacVisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])
        self.gradient_checkpointing = False

    # Ignore copy
    @can_return_tuple
    @check_model_inputs
    def forward(
        self,
        inputs_embeds,
        attention_mask: Optional[torch.Tensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutput:
        hidden_states = inputs_embeds
        for encoder_layer in self.layers:
            hidden_states = encoder_layer(
                hidden_states,
                attention_mask,
                **kwargs,
            )
        return BaseModelOutput(last_hidden_state=hidden_states)


def document_mask_function_from_cu_seqlens(cu_seqlens: Optional[torch.Tensor]) -> Optional[Callable]:
    """Return a mask function that blocks cross-document attention from packed ``cu_seqlens``.

    The returned callable matches the signature expected by ``masking_utils`` mask factories and
    yields ``True`` only when query/key positions belong to the same packed segment.
    """

    if cu_seqlens is None:
        return None

    if cu_seqlens.numel() < 2:
        return None

    seq_sizes = (cu_seqlens[1:] - cu_seqlens[:-1]).long()
    if seq_sizes.numel() == 0:
        return None

    total_tokens = int(seq_sizes.sum().item())
    seg_ids = torch.repeat_interleave(torch.arange(seq_sizes.numel(), device=cu_seqlens.device), seq_sizes)
    packed_sequence_mask = seg_ids.view(1, total_tokens)
    return packed_sequence_mask_function(packed_sequence_mask)


def create_document_attention_mask(
    config: PretrainedConfig,
    input_embeds: torch.Tensor,
    cu_seqlens: Optional[torch.Tensor],
) -> Optional[Union[torch.Tensor, Any]]:
    """Materialize a backend-specific block-diagonal attention mask.

    This uses the standard `masking_utils` mask interface (same mechanism as Llama4),
    so the returned object matches the selected attention backend (e.g. SDPA bool mask,
    eager additive mask, or flex `BlockMask`).
    """

    mask_function = document_mask_function_from_cu_seqlens(cu_seqlens)
    if mask_function is None:
        return None

    seq_len = input_embeds.shape[1]
    cache_position = torch.arange(seq_len, device=input_embeds.device, dtype=torch.long)

    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]
    return mask_interface(
        batch_size=input_embeds.shape[0],
        cache_position=cache_position,
        kv_length=seq_len,
        kv_offset=0,
        mask_function=mask_function,
        attention_mask=None,
        allow_is_causal_skip=False,
        allow_is_bidirectional_skip=False,
        dtype=input_embeds.dtype,
        config=config,
        use_vmap=False,
    )


def create_pixel_shuffle_index_map(
    seq_sizes: torch.Tensor,
    token_grids: torch.Tensor,
    scale_factor: int = 1,
    device: Optional[torch.device] = None,
) -> torch.Tensor:
    """
    Build a gather-index map that tells us, for every *output* token after
    pixel-shuffle, which `scale_factor**2` *input* tokens are being merged.

    Args
    ----
    seq_sizes     : (num_images,)  - #patches in each image (row-major order)
    token_grids   : (num_images,2) - (height, width) for every image
    scale_factor  : spatial down-scale factor (â‰¥2)
    device        : (optional) overrides `seq_sizes.device`

    Returns
    -------
    gather_idx : (new_total_seq_len, scale_factor**2) int64 tensor.
                 gather_idx[i, j] is the *flat* index into the *original*
                 packed sequence for the j-th sub-patch that forms the
                 i-th output token.
    """
    if device is None:
        device = seq_sizes.device

    scale_factor = int(scale_factor)
    if scale_factor < 2:
        raise ValueError("`scale_factor` must be â‰¥ 2")

    # Safety: all spatial dims must be divisible by the scale factor
    # Cannot run under torch compile fullgraph mode hence
    if not is_torchdynamo_compiling():
        if not ((token_grids[:, 0] % scale_factor == 0).all() and (token_grids[:, 1] % scale_factor == 0).all()):
            raise AssertionError(
                "Every (H,W) in `token_grids` must be divisible by "
                f"scale_factor={scale_factor}, got {token_grids.tolist()}"
            )

    gather_chunks: list[torch.Tensor] = []
    tok_offset = 0

    for seq_len, (h, w) in zip(seq_sizes.tolist(), token_grids.tolist(), strict=False):
        # Build the (H, W) grid of flat indices for this image
        grid = torch.arange(seq_len, device=device, dtype=torch.int64) + tok_offset
        grid = grid.view(h, w)  # (H, W)

        # -------- identical ordering to your fixed-res routine --------
        # Step 1: split width into blocks of scale_factor
        grid = grid.view(h, w // scale_factor, scale_factor)  # (H, W/scale_factor, scale_factor)
        # Step 2: now split height into blocks of scale_factor
        grid = grid.view(h // scale_factor, scale_factor, w // scale_factor, scale_factor)
        # (H/scale_factor, scale_factor, W/scale_factor, scale_factor)
        # Step 3: final permutation to (H/scale_factor, W/scale_factor, scale_factor, scale_factor)
        grid = grid.permute(0, 2, 1, 3).contiguous()  # (H/scale_factor, W/scale_factor, scale_factor, scale_factor)
        # Step 4: each (scale_factor, scale_factor) block forms one output token
        gather_chunks.append(grid.reshape(-1, scale_factor * scale_factor))
        # (H*W / scale_factor**2, scale_factor**2)

        tok_offset += seq_len

    # Concatenate over all images in the packed batch
    gather_idx = torch.cat(gather_chunks, dim=0)  # (Î£_i Háµ¢Wáµ¢/scale_factor**2, scale_factor**2)
    return gather_idx


def pixel_shuffle_varlen(
    x: torch.Tensor,
    token_grids: torch.Tensor,
    scale_factor: int = 1,
) -> torch.Tensor:
    r"""Apply pixel shuffle to a packed vision sequence without unpacking per image.

    Args:
        x (`torch.Tensor`):
            Concatenated vision embeddings. Accepts `(seq_len, hidden_size)` or `(1, seq_len, hidden_size)` shapes
            produced by stacking image patches.
        token_grids (`torch.Tensor`):
            Integer tensor of shape `(num_images, 2)` whose rows give the `(height, width)` patch grid sizes
            corresponding to each image segment inside `x`.
        scale_factor (`int`, *optional*, defaults to 1):
            Spatial down-sampling factor specific to pixel shuffle. Values greater than one merge `scale_factor**2` neighboring patches into a
            single embedding channel-group.

    Returns:
        `torch.Tensor`: Pixel-shuffled embeddings with shape matching the input convention:
        `(seq_len, hidden_size * scale_factor**2)` when the input was 2D, or `(1, seq_len, hidden_size * scale_factor**2)`
        if the singleton batch dimension was present.

    Raises:
        ValueError: If more than one batch item is provided.
    """
    return_with_batch_dim = x.dim() == 3
    if return_with_batch_dim:
        if x.size(0) != 1:
            raise AssertionError("Packed sequence is expected to have batch_size == 1")
        embeddings = x.squeeze(0)  # (seq, embed)
    else:
        embeddings = x  # (seq, embed)

    embed_dim = embeddings.size(-1)
    scale_factor = int(scale_factor)

    # Calculate seq_sizes from token_grids
    seq_sizes = torch.prod(token_grids, dim=-1)

    # Build index map and gather in one go
    gather_idx = create_pixel_shuffle_index_map(
        seq_sizes=seq_sizes,
        token_grids=token_grids,
        scale_factor=scale_factor,
        device=embeddings.device,
    )  # (new_seq, scale_factor**2)

    # Gather â†’ (new_seq, scale_factor**2, embed_dim)
    gathered = embeddings[gather_idx]  # fancy indexing keeps gradient

    # Merge the scale_factor**2 group dimension into channels to finish the shuffle
    out = gathered.reshape(gathered.size(0), embed_dim * scale_factor * scale_factor)

    # Restore batch dimension if needed
    if return_with_batch_dim:
        out = out.unsqueeze(0)
    return out


class IsaacVisionTransformer(nn.Module):
    _supports_sdpa = True

    def __init__(self, config: IsaacVisionConfig):
        super().__init__()
        self.config = config
        self.embeddings = IsaacVisionEmbeddings(config)
        self.encoder = IsaacVisionEncoder(config)
        self.post_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.pixel_shuffle_scale_factor = config.pixel_shuffle_scale_factor

    def forward(self, packed_seq_patches: tuple[torch.Tensor, torch.Tensor]):
        seq_patches, token_grids = packed_seq_patches
        seq_sizes = torch.prod(token_grids, dim=-1)

        # Get embeddings from packed sequence
        hidden_states = self.embeddings(seq_patches, token_grids)

        # Add a pseudo batch dimension for the encoder
        hidden_states = hidden_states.unsqueeze(0)

        # Generate cumulative sequence lengths for variable-length attention
        cu_seqlens = torch.zeros(seq_sizes.size(0) + 1, dtype=torch.int32, device=hidden_states.device)
        cu_seqlens[1:] = seq_sizes.cumsum(0)

        attention_mask = create_document_attention_mask(self.config, hidden_states, cu_seqlens)

        # Pass through encoder with variable-length attention parameters
        encoder_outputs = self.encoder(
            inputs_embeds=hidden_states,
            attention_mask=attention_mask,
            cu_seqlens=cu_seqlens,
        )
        hidden_states = encoder_outputs.last_hidden_state

        # Apply final layer normalization
        hidden_states = self.post_layernorm(hidden_states)

        hidden_states = pixel_shuffle_varlen(
            x=hidden_states,
            token_grids=token_grids,
            scale_factor=self.pixel_shuffle_scale_factor,
        )
        # Remove the pseudo batch dimension we added earlier
        hidden_states = hidden_states.squeeze(0)

        # Return the full sequence of embeddings
        return hidden_states


class IsaacMultiModalProjector(nn.Module):
    def __init__(self, config: IsaacConfig):
        super().__init__()
        self.vision_hidden_size = config.vision_config.hidden_size * (
            config.vision_config.pixel_shuffle_scale_factor**2
        )
        self.backbone_hidden_size = config.hidden_size
        self.linear_1 = nn.Linear(self.vision_hidden_size, 4 * self.vision_hidden_size, bias=False)
        self.silu = nn.SiLU()
        self.linear_2 = nn.Linear(4 * self.vision_hidden_size, self.backbone_hidden_size, bias=False)

    def forward(self, image_features):
        hidden_states = self.linear_1(image_features)
        hidden_states = self.silu(hidden_states)
        hidden_states = self.linear_2(hidden_states)
        return hidden_states


class IsaacVisionEmbedding(nn.Module):
    """Vision embedding wrapper exposing tower and projector."""

    _supports_sdpa = True

    def __init__(self, config: IsaacConfig):
        super().__init__()
        vision_cfg = config.vision_config

        self.vision_tower = IsaacVisionTransformer(vision_cfg)
        self.multimodal_projector = IsaacMultiModalProjector(config)

    def forward(self, vision_tokens: tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:
        hidden_states = self.vision_tower(vision_tokens)
        return self.multimodal_projector(hidden_states)


class IsaacRotaryEmbedding(qwen2_5_vl_modeling.Qwen2_5_VLRotaryEmbedding):
    EXTRA_ROPE_KEYS = {"mrope_section", "mrope_interleaved"}

    def __init__(self, config: IsaacConfig, device=None):
        rope_source_cfg = config.get_text_config() if hasattr(config, "get_text_config") else config
        rope_scaling = getattr(rope_source_cfg, "rope_scaling", None) or {}

        sanitized_scaling = {k: v for k, v in rope_scaling.items() if k not in self.EXTRA_ROPE_KEYS}
        config_for_rope = copy.copy(rope_source_cfg)
        config_for_rope.rope_scaling = sanitized_scaling if sanitized_scaling else None

        init_device = device if device is not None and getattr(device, "type", None) != "meta" else None
        super().__init__(config_for_rope, device=init_device)

        rotary_half_dim = self.inv_freq.shape[0]
        self.mrope_section = self._resolve_mrope_section(rope_scaling.get("mrope_section"), rotary_half_dim)
        self.hidden_size = getattr(rope_source_cfg, "hidden_size", None) or config.hidden_size

    @staticmethod
    def _resolve_mrope_section(section: Optional[list[int]], rotary_half_dim: int) -> list[int]:
        if section is None:
            weights = (2, 1, 1)
            base = [rotary_half_dim * w // sum(weights) for w in weights]
            base[0] += rotary_half_dim - sum(base)
            return base

        section = [int(v) for v in section]
        if len(section) != 3:
            raise ValueError("`mrope_section` must contain exactly three elements (temporal, height, width)")
        if sum(section) != rotary_half_dim:
            raise ValueError(
                f"`mrope_section` must sum to the rotary half-dimension ({rotary_half_dim}). Received {section}."
            )
        return section

    def _combine_axes(self, tensor: torch.Tensor) -> torch.Tensor:
        split_sections = tuple(self.mrope_section * 2)
        chunks = tensor.split(split_sections, dim=-1)
        return torch.cat([chunk[i % 3] for i, chunk in enumerate(chunks)], dim=-1)

    def forward(
        self,
        position_ids: torch.Tensor,
        modality_tensor: torch.Tensor,
        hidden_states: Optional[torch.Tensor] = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        if position_ids.ndim != 3 or position_ids.size(-1) != 3:
            raise ValueError("`position_ids` must have shape (batch, seq_len, 3) for MRoPE")
        if modality_tensor.shape != position_ids.shape[:2]:
            raise ValueError("`modality_tensor` must align with the first two dims of `position_ids`")

        if hidden_states is None:
            batch, seq_len, _ = position_ids.shape
            hidden_states = torch.zeros(
                batch,
                seq_len,
                self.hidden_size,
                dtype=torch.float32,
                device=position_ids.device,
            )

        with torch.no_grad():
            pos = position_ids.clone()
            image_value = ModalityType.image.value
            not_spatial = modality_tensor != image_value
            if not_spatial.any():
                data_1d = pos[not_spatial][..., 0].unsqueeze(-1)
                pos[not_spatial] = data_1d.expand(-1, pos.shape[-1])

            pos_axes = pos.permute(2, 0, 1).contiguous()

        cos_axes, sin_axes = super().forward(hidden_states, pos_axes)

        cos_axes = cos_axes.to(hidden_states.dtype)
        sin_axes = sin_axes.to(hidden_states.dtype)

        cos_combined = self._combine_axes(cos_axes)
        sin_combined = self._combine_axes(sin_axes)

        return cos_combined, sin_combined


def _infer_device_from_streams(streams: list[Stream]) -> torch.device | str:
    return next(
        (ev.data.device for stream in streams for ev in stream.events if isinstance(ev.data, torch.Tensor)),
        "cpu",
    )


def _compact_stream_events(stream: Stream) -> torch.Tensor:
    assert all([(isinstance(ev.data, torch.Tensor) and ev.is_measured) for ev in stream.events]), (
        "stream_apply(compact=True) only works for streams with events that have measured tensor data"
    )
    return torch.cat([ev.data for ev in stream.events]).contiguous()


def _map_stream(
    stream: Stream,
    map_fn: Callable[[Event], Any] | None,
    *,
    copy_unchanged: bool,
) -> tuple[str, Stream | list[Any]]:
    if map_fn is None:
        return "events", stream if not copy_unchanged else stream.shallow_copy()

    mapped_events: list[Event] = []
    flat_values: list[Any] = []
    mode: str | None = None

    for ev in stream:
        out = map_fn(ev)
        if out is None:
            out = {}

        if isinstance(out, dict):
            if mode is None:
                mode = "events"
            elif mode != "events":
                raise ValueError("map_fn must consistently return a dict or an iterable for every event")

            if not out:
                mapped_events.append(ev if not copy_unchanged else ev.shallow_copy())
                continue

            new_ev = ev.shallow_copy()
            for k, v in out.items():
                setattr(new_ev, k, v)
            mapped_events.append(new_ev)
            continue

        if isinstance(out, Iterable) and not isinstance(out, (str, bytes)):
            if mode is None:
                mode = "flat"
            elif mode != "flat":
                raise ValueError("map_fn must consistently return a dict or an iterable for every event")

            flat_values.extend(out)
            continue

        raise TypeError(f"map_fn must return a dict or iterable, got {type(out)}")

    if mode is None:
        mode = "events"
        mapped_events = stream.events if not copy_unchanged else [ev.shallow_copy() for ev in stream.events]
        return mode, Stream(mapped_events, priority=stream.priority)

    if mode == "events":
        return mode, Stream(mapped_events, priority=stream.priority)

    return mode, flat_values


def stream_apply(
    stream_like: Stream | TensorStream,
    map_fn: Callable[[Event], Any] | None = None,
    *,
    compact: bool = False,
    copy_unchanged: bool = False,
) -> Stream | TensorStream | torch.Tensor | list[Any] | list[list[Any]]:
    """
    Unified helper to (optionally) map over events and (optionally) compact to tensors.

    Args:
        stream_like: A ``Stream`` or ``TensorStream`` instance.
        map_fn: Optional callable applied to every ``Event``. It may return a dict of field deltas
            (structural map) or an iterable of values (token map). Returning ``None`` is treated as ``{}``.
        compact: When True, returns a tensor (``(B, T)`` for ``TensorStream``, ``(T,)`` for ``Stream``).
        copy_unchanged: When True, shallow-copy events even if unchanged.

    Returns:
        Stream/TensorStream, torch.Tensor, or list(s) depending on ``compact`` and ``map_fn`` mode.
    """
    is_batch = isinstance(stream_like, TensorStream)

    streams = stream_like.streams if is_batch else [stream_like]
    mapped_streams: list[Stream] = []
    flat_batches: list[list[Any]] = []
    mode: str | None = None

    for stream in streams:
        stream_mode, payload = _map_stream(stream, map_fn, copy_unchanged=copy_unchanged)
        if mode is None:
            mode = stream_mode
        elif mode != stream_mode:
            raise ValueError("map_fn must return the same type (dict vs iterable) for every stream")

        if stream_mode == "events":
            mapped_streams.append(payload)  # type: ignore[arg-type]
        else:
            flat_batches.append(payload)  # type: ignore[arg-type]

    if mode is None:
        mode = "events"
        mapped_streams = streams if not copy_unchanged else [s.shallow_copy() for s in streams]

    if not compact:
        if mode == "events":
            return TensorStream(mapped_streams) if is_batch else mapped_streams[0]
        return flat_batches if is_batch else flat_batches[0]

    if mode == "events":
        compacted = [_compact_stream_events(s) for s in mapped_streams]
        return torch.stack(compacted).contiguous() if is_batch else compacted[0]

    device = _infer_device_from_streams(streams)
    flat_values: list[Any] = list(itertools.chain.from_iterable(flat_batches)) if is_batch else flat_batches[0]
    tensor = torch.tensor(flat_values, dtype=torch.long, device=device)
    if is_batch:
        B, T = stream_like.shape  # type: ignore[union-attr]
        tensor = tensor.reshape(B, T)
    return tensor.contiguous()


def compute_mrope_pos_tensor(ts: TensorStream, n_pos_dims: int = 3) -> torch.Tensor:
    """
    Create a (batch, T, n_pos_dims) position tensor in one sweep.
    The first dim is the running â€œtimeâ€ index, the rest are spatial (or 1-fillers).

    Args:
        ts         : TensorStream
        n_pos_dims : total coordinate dimensions (default 3)

    Returns:
        torch.LongTensor  - shape (batch_size, seq_len, n_pos_dims)
    """

    # Manually iterate through streams and events like map_compact does,
    # but maintain cumulative time offset for each stream
    all_coords = []
    for stream in ts.streams:  # one Stream == one batch sample
        cumulative_offset = 0  # running time index for this stream

        for event in stream:
            # --- build coordinate grid for THIS event using itertools (no tensor ops) ---
            dims = (event.dims() or [1]) + [1] * (n_pos_dims - len(event.dims() or []))

            # Create ranges for each dimension (similar to old _finalize implementation)
            first_dim = range(cumulative_offset, cumulative_offset + dims[0])
            cumulative_offset += dims[0]  # advance time for the next event
            other_dims = [range(d) for d in dims[1:]]

            # Use itertools.product to create all coordinate combinations
            full_coords = list(itertools.product(first_dim, *other_dims))

            # Slice if the event is partial
            s, e = event.idx_range
            coords = full_coords[s:e]

            # Extend the flattened coordinate list
            all_coords.extend(coords)

    # Convert to tensor and reshape to (B, T, n_pos_dims)
    B, T = ts.shape
    return torch.tensor(all_coords, dtype=torch.long, device=ts.device).reshape(B, T, n_pos_dims)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Generic event-labelling helper
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def event_mask(
    ts: TensorStream,
    tag_fn: Callable[[Event], int | None],
    default: int = -1,
) -> torch.Tensor:
    """
    Build a (batch, seq_len) LongTensor whose value for every *token*
    is given by `tag_fn(event)`, falling back to `default` when the
    function returns None.

    The work is done in a single pass via `map  â†’  compact`.
    """

    def to_label(ev: Event) -> Any:
        label = tag_fn(ev)
        if label is None:
            label = default
        return [label] * ev.num_tokens()

    return stream_apply(ts, to_label, compact=True).squeeze(-1)


def tensor_stream_token_view(
    text_token_ids: torch.Tensor,
    modality_tensor: torch.Tensor,
    *,
    fill_value: int = 0,
) -> torch.Tensor:
    """
    Compose a (batch, seq_len) token tensor using text token ids and a modality map.
    Non-text positions are filled with ``fill_value``.
    """
    if modality_tensor.dim() != 2:
        raise ValueError("`modality_tensor` must be 2D (batch, seq_len).")

    if text_token_ids.dim() == 1:
        text_token_ids = text_token_ids.unsqueeze(0)
    if text_token_ids.dim() != 2:
        raise ValueError("`text_token_ids` must be 1D or 2D.")

    batch_size, seq_len = modality_tensor.shape
    token_batch = text_token_ids.shape[0]
    if token_batch not in {1, batch_size}:
        raise ValueError("`text_token_ids` batch dimension must match `modality_tensor` or be 1.")

    if token_batch == 1 and batch_size > 1:
        text_token_ids = text_token_ids.expand(batch_size, -1)

    text_mask = modality_tensor == ModalityType.text.value
    text_counts = text_mask.sum(dim=1)
    max_text_needed = int(text_counts.max().item()) if text_counts.numel() else 0

    if text_token_ids.size(1) < max_text_needed:
        raise ValueError("`text_token_ids` does not have enough tokens for the provided modalities.")

    tokens = modality_tensor.new_full(
        (batch_size, seq_len), fill_value, dtype=torch.long, device=modality_tensor.device
    )
    for batch_idx in range(batch_size):
        needed = int(text_counts[batch_idx].item())
        if needed == 0:
            continue
        tokens[batch_idx, text_mask[batch_idx]] = text_token_ids[batch_idx, :needed].to(
            device=tokens.device, dtype=torch.long
        )

    return tokens


def tensor_stream_to_packed_inputs(tensor_stream: TensorStream) -> dict[str, Optional[torch.Tensor]]:
    """
    Extract plain tensor payloads from a TensorStream so downstream code can start
    bypassing the Event/Stream abstraction. The returned tensors cover the Event
    fields we previously relied on:

    - modality_tensor mirrors `Event.type` for every token.
    - position_ids materialize MRoPE-ready coordinates from `Event.idx_range` and `Event.dims`.
    - vision_token_grids store the real spatial grid from `Event.dims(real)`.
    - vision_token_offsets/vision_token_lengths mirror `Event.idx_range` for vision events after truncation.
    - vision_patches keeps `Event.data` (real patch vectors) concatenated.
    - text_token_ids preserves the text token payloads (batch, num_text_tokens).
    """

    device = tensor_stream.device or torch.device(_infer_device_from_streams(tensor_stream.streams))

    text_token_batches: list[torch.Tensor] = []
    for stream in tensor_stream.streams:
        stream_tokens: list[torch.Tensor] = []
        for ev in stream:
            if ev.type != ModalityType.text:
                continue
            tokens = ev.data
            if tokens.dim() > 1:
                tokens = tokens.reshape(-1)
            if ev.idx_range is not None:
                start, end = ev.idx_range
                tokens = tokens[start:end]
            stream_tokens.append(tokens.to(device=device, dtype=torch.long))
        if stream_tokens:
            text_token_batches.append(torch.cat(stream_tokens, dim=0))
        else:
            text_token_batches.append(torch.zeros((0,), device=device, dtype=torch.long))

    max_text_len = max(tokens.numel() for tokens in text_token_batches) if text_token_batches else 0
    text_token_ids = torch.zeros((len(text_token_batches), max_text_len), device=device, dtype=torch.long)
    for idx, tokens in enumerate(text_token_batches):
        if tokens.numel():
            text_token_ids[idx, : tokens.numel()] = tokens

    vision_events = [ev for stream in tensor_stream.streams for ev in stream if ev.type == ModalityType.image]

    seq_patches: Optional[torch.Tensor]
    token_grids: Optional[torch.Tensor]
    token_offsets: Optional[torch.Tensor]
    token_lengths: Optional[torch.Tensor]

    if vision_events:
        patch_chunks: list[torch.Tensor] = []
        grid_rows: list[tuple[int, int]] = []
        offset_rows: list[int] = []
        length_rows: list[int] = []

        for ev in vision_events:
            event_data = ev.data
            if event_data.dim() > 2:
                event_data = event_data.view(event_data.size(0), -1)

            # idx_range describes how much of this event survives truncation
            token_span = ev.idx_range
            if token_span is None:
                token_span = (0, math.prod(ev.dims() or [event_data.shape[0]]))
            start, end = token_span

            patch_chunks.append(event_data)
            dims_real = ev.dims(False) or ev.dims() or [event_data.shape[0], 1, 1]
            if len(dims_real) < 3:
                dims_real = list(dims_real) + [1] * (3 - len(dims_real))
            grid_rows.append((int(dims_real[1]), int(dims_real[2])))
            offset_rows.append(int(max(0, start)))
            length_rows.append(int(max(0, end - start)))

        seq_patches = torch.cat(patch_chunks, dim=0)
        token_grids = torch.tensor(grid_rows, dtype=torch.long, device=seq_patches.device)
        token_offsets = torch.tensor(offset_rows, dtype=torch.long, device=seq_patches.device)
        token_lengths = torch.tensor(length_rows, dtype=torch.long, device=seq_patches.device)
    else:
        seq_patches = None
        token_grids = None
        token_offsets = None
        token_lengths = None

    modality_tensor = event_mask(tensor_stream, lambda ev: ev.type.value)

    return {
        "vision_patches": seq_patches,
        "vision_token_grids": token_grids,
        "vision_token_offsets": token_offsets,
        "vision_token_lengths": token_lengths,
        "modality_tensor": modality_tensor,
        "position_ids": compute_mrope_pos_tensor(tensor_stream),
        "text_token_ids": text_token_ids,
    }


# ============================================================================
# Model
# ============================================================================


def compute_position_ids_input_ids(input_ids: torch.Tensor) -> torch.Tensor:
    r"""Create 3D positional indices for token input.

    Args:
        input_ids (`torch.Tensor`):
            Tensor of shape `(batch_size, seq_len)` containing token ids.

    Returns:
        `torch.Tensor`: Positional indices with shape `(batch_size, seq_len, 3)` where each channel duplicates the
        1D position so it can be consumed by the 3-axis MRoPE rotary embedding.
    """
    batch_size, seq_length = input_ids.shape
    position_ids = torch.arange(seq_length, device=input_ids.device)
    position_ids = position_ids.view(1, -1).expand(batch_size, -1)
    position_ids = position_ids.unsqueeze(2).expand(-1, -1, 3)  # Add 3D for MRoPE
    return position_ids


@auto_docstring
class IsaacModel(PreTrainedModel):
    config: IsaacConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["IsaacDecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = False
    _can_compile_fullgraph = False
    _supports_attention_backend = True
    _can_record_outputs = {"attentions": OutputRecorder(IsaacVisionAttention, index=1)}
    # Expose tied-weights mapping even if empty for base model tests.
    all_tied_weights_keys: dict[str, str] = {}

    def __init__(self, config: IsaacConfig):
        Qwen3PreTrainedModel.__init__(self, config)

        text_cfg_source = config.text_config
        text_cfg = copy.deepcopy(text_cfg_source)
        self.text_model = AutoModel.from_config(text_cfg)
        # Ensure downstream callers observe the composed config
        self.text_model.config = config

        self.rotary_emb = IsaacRotaryEmbedding(config, device=self.device)

        if config.vision_config is None:
            raise ValueError("IsaacConfig should always have vision_config")

        self.vision_embedding = IsaacVisionEmbedding(config)
        self.vision_embedding._supports_sdpa = True

        # Dispatch table for TensorStream balanced embedding (text + vision)
        self.embed_fns = {
            ModalityType.text: self.embed_text_tokens,
            ModalityType.padding: self.embed_text_tokens,
            ModalityType.image: self.embed_vision,
        }

        # Keep track of config attributes that downstream utilities may query directly on the model.
        self.max_sequence_length = config.max_sequence_length
        self.vision_rescale_factor = config.vision_rescale_factor
        self.vision_token = config.vision_token

        # Initialize weights and parallel plans (including tp_plan from the text model)
        self.post_init()

        # Respect config-specified gradient checkpointing
        if getattr(config, "gradient_checkpointing", False):
            self.gradient_checkpointing_enable()

    def get_input_embeddings(self) -> nn.Module:
        return self.text_model.get_input_embeddings()

    def set_input_embeddings(self, value: nn.Module) -> None:
        self.text_model.set_input_embeddings(value)
        vocab_size = getattr(value, "num_embeddings", None)
        if vocab_size is not None:
            self.config.vocab_size = vocab_size
            if hasattr(self.config, "text_config"):
                self.config.text_config.vocab_size = vocab_size
            self.text_model.config.vocab_size = vocab_size

    @property
    def embed_tokens(self) -> nn.Module:
        return self.text_model.embed_tokens

    @embed_tokens.setter
    def embed_tokens(self, value: nn.Module) -> None:
        self.text_model.embed_tokens = value

    @property
    def vision_model(self) -> nn.Module:
        return self.vision_embedding.vision_tower

    def embed_text_tokens(self, token_ids: torch.Tensor) -> torch.Tensor:
        """Embed text tokens, squeezing singleton dimensions."""
        # Text events are shaped as (..., 1); squeeze the singleton index dim
        h = self.text_model.embed_tokens(token_ids)
        if h.dim() >= 2 and h.size(-2) == 1:
            h = h[..., 0, :]
        return h

    def embed_vision(self, vision_tokens: tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:
        """Embed vision tokens using the vision encoder."""
        # vision tokens is (seq_patches, token_grids)
        return self.vision_embedding(vision_tokens)

    def embed_packed_inputs(
        self, input_ids: torch.Tensor, packed_inputs: dict[str, Optional[torch.Tensor]]
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Embed packed_inputs (plain tensors) instead of TensorStream.

        Expects input_ids for text tokens and packed_inputs containing:
        - modality_tensor: (batch, seq_len) modality ids aligned to the sequence
        - position_ids: (batch, seq_len, 3) MRoPE coordinates (optional)
        - vision_patches: concatenated vision tokens shaped (total_tokens, embed_dim) or None
        - vision_token_grids: (num_images, 2) token grid sizes or None
        - vision_token_offsets: (num_images,) offsets into each image's virtual token span (optional)
        - vision_token_lengths: (num_images,) surviving virtual token lengths per image (optional)
        """

        if input_ids is None:
            raise ValueError("`input_ids` must be provided when using `packed_inputs`.")

        modality_tensor = packed_inputs.get("modality_tensor")
        if modality_tensor is None:
            modality_tensor = torch.full_like(input_ids, ModalityType.text.value, dtype=torch.long)
        else:
            modality_tensor = modality_tensor.to(device=input_ids.device, dtype=torch.long)

        text_embeds = self.embed_text_tokens(input_ids)
        embeds = text_embeds

        vision_patches = packed_inputs.get("vision_patches")
        if vision_patches is not None:
            token_grids = packed_inputs.get("vision_token_grids")
            if token_grids is None:
                raise ValueError("`vision_token_grids` must accompany `vision_patches` in packed_inputs.`")

            vision_token_offsets = packed_inputs.get("vision_token_offsets")
            vision_token_lengths = packed_inputs.get("vision_token_lengths")

            vision_embeds = self.embed_vision((vision_patches, token_grids))

            vision_seq_sizes = torch.prod(token_grids.to(device=vision_embeds.device), dim=-1)
            scale_factor = getattr(self.config.vision_config, "pixel_shuffle_scale_factor", 1)
            if scale_factor > 1:
                vision_seq_sizes = vision_seq_sizes // int(scale_factor * scale_factor)

            if vision_token_offsets is not None and vision_token_offsets.numel() != vision_seq_sizes.numel():
                raise ValueError(
                    "`vision_token_offsets` must match number of images inferred from vision_token_grids.`"
                )
            if vision_token_lengths is not None and vision_token_lengths.numel() != vision_seq_sizes.numel():
                raise ValueError(
                    "`vision_token_lengths` must match number of images inferred from vision_token_grids.`"
                )

            if vision_token_offsets is not None or vision_token_lengths is not None:
                offsets = vision_token_offsets
                lengths = vision_token_lengths
                if offsets is None:
                    offsets = torch.zeros_like(vision_seq_sizes, dtype=torch.long, device=vision_seq_sizes.device)
                else:
                    offsets = offsets.to(device=vision_embeds.device, dtype=torch.long)
                if lengths is None:
                    lengths = vision_seq_sizes.to(device=vision_embeds.device)
                else:
                    lengths = lengths.to(device=vision_embeds.device, dtype=torch.long)

                selected_chunks: list[torch.Tensor] = []
                cursor = 0
                for seq_len, offset, length in zip(vision_seq_sizes.tolist(), offsets.tolist(), lengths.tolist()):
                    end = cursor + seq_len
                    if seq_len == 0:
                        cursor = end
                        continue
                    chunk = vision_embeds[cursor:end]
                    chunk_offset = max(0, min(offset, seq_len))
                    chunk_length = max(0, min(length, seq_len - chunk_offset))
                    selected_chunks.append(chunk[chunk_offset : chunk_offset + chunk_length])
                    cursor = end

                vision_embeds = (
                    torch.cat(selected_chunks, dim=0)
                    if selected_chunks
                    else vision_embeds.new_zeros((0, vision_embeds.size(-1)))
                )

            vision_mask = modality_tensor == ModalityType.image.value
            expected_image_tokens = int(vision_mask.sum().item())
            if expected_image_tokens != vision_embeds.shape[0]:
                raise ValueError("Packed vision payload size does not match modality tensor.")

            embeds = embeds.clone()
            embeds[vision_mask] = vision_embeds.to(embeds.device)

        return embeds, modality_tensor

    @staticmethod
    def compute_position_ids_input_ids(input_ids: torch.Tensor) -> torch.Tensor:
        return compute_position_ids_input_ids(input_ids)

    def _prepare_position_and_modality(
        self,
        position_ids: Optional[torch.LongTensor],
        modality_tensor: Optional[torch.LongTensor],
        inputs_embeds: torch.Tensor,
        cache_position: torch.LongTensor,
    ) -> tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor, torch.Tensor, torch.Tensor]:
        text_value = ModalityType.text.value
        batch_size, seq_len = inputs_embeds.shape[:2]

        if modality_tensor is None:
            modality_tensor = torch.full(
                (batch_size, seq_len), text_value, device=inputs_embeds.device, dtype=torch.long
            )
        else:
            modality_tensor = modality_tensor.to(device=inputs_embeds.device, dtype=torch.long)
            expected_shape = (batch_size, seq_len)
            if modality_tensor.shape != torch.Size(expected_shape):
                raise ValueError(
                    f"modality_tensor must have shape (batch_size, seq_len) {expected_shape}, "
                    f"but got {tuple(modality_tensor.shape)}"
                )

        if position_ids is None:
            position_ids = cache_position.view(1, -1).expand(modality_tensor.shape[0], -1)

        if position_ids.ndim == 2:
            position_ids = position_ids.to(device=inputs_embeds.device)
            position_ids = position_ids.unsqueeze(-1).expand(-1, -1, 3)

        if position_ids.shape[1] != seq_len:
            start_positions = position_ids[:, :1, 0]
            position_ids = torch.arange(seq_len, device=inputs_embeds.device).view(1, -1)
            position_ids = position_ids + start_positions
            position_ids = position_ids.unsqueeze(-1).expand(-1, -1, 3)

        cos, sin = self.rotary_emb(
            position_ids,
            modality_tensor,
            hidden_states=inputs_embeds,
        )

        decoder_position_ids = position_ids[..., 0] if position_ids.ndim == 3 else position_ids
        return position_ids, modality_tensor, decoder_position_ids, cos, sin

    @auto_docstring
    @check_model_inputs
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        tensor_stream: Optional[TensorStream] = None,
        packed_inputs: Optional[dict[str, torch.Tensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        modality_tensor: Optional[torch.LongTensor] = None,
        past_key_values: Optional[list[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple | BaseModelOutputWithPast:
        """
        Forward pass with MRoPE position embeddings.

        Computes position embeddings once and passes them through all layers.

        Args:
            tensor_stream (`TensorStream`, *optional*):
                Packed multimodal stream of text and vision events to embed directly. Mutually exclusive with
                `input_ids` and `inputs_embeds`. When provided, the method derives `position_ids` and `modality_tensor`
                if they are not supplied.
            packed_inputs (`dict`, *optional*):
                Plain tensor payloads extracted from a TensorStream. When provided, it replaces the TensorStream path
                and requires `input_ids` for text tokens.
            modality_tensor (`torch.LongTensor`, *optional*):
                Modality identifiers aligned with the embedded sequence, shaped `(batch_size, seq_len)` and containing
                values from `ModalityType`. Automatically built from `tensor_stream` or `input_ids` when
                omitted.
        """

        output_attentions = kwargs.pop("output_attentions", None)
        packed_inputs = None

        converted_from_stream = False
        if tensor_stream is not None and packed_inputs is None:
            packed_inputs = tensor_stream_to_packed_inputs(tensor_stream)
            text_token_ids = packed_inputs.get("text_token_ids")
            tensor_stream = None

            if position_ids is None:
                position_ids = packed_inputs.get("position_ids")

            if input_ids is None:
                if text_token_ids is None:
                    raise ValueError("`text_token_ids` is required when converting from `tensor_stream`.")

                modality_for_ids = packed_inputs.get("modality_tensor")
                if modality_for_ids is None:
                    raise ValueError("`modality_tensor` is required when converting from `tensor_stream`.")

                fill_value = getattr(getattr(self.config, "text_config", self.config), "pad_token_id", None)
                if fill_value is None or fill_value < 0:
                    fill_value = 0

                input_ids = tensor_stream_token_view(text_token_ids, modality_for_ids, fill_value=fill_value).to(
                    dtype=torch.long
                )
                modality_for_ids = modality_for_ids.to(device=input_ids.device, dtype=torch.long)
                image_mask = modality_for_ids == ModalityType.image.value
                if image_mask.any():
                    safe_token_id = getattr(getattr(self.config, "text_config", self.config), "pad_token_id", None)
                    if safe_token_id is None:
                        safe_token_id = getattr(self.config, "pad_token_id", None)
                    if safe_token_id is None or safe_token_id < 0:
                        safe_token_id = int(self.config.vocab_size - 1)
                    input_ids = input_ids.clone()
                    input_ids[image_mask] = safe_token_id

            converted_from_stream = True

        if packed_inputs is not None and inputs_embeds is not None and not converted_from_stream:
            raise ValueError("`inputs_embeds` should not be provided alongside `packed_inputs`.")

        # Resolve the input source (prefer packed_inputs > tensor_stream > ids > embeds).
        precomputed_modality: Optional[torch.Tensor] = None
        precomputed_position_ids: Optional[torch.Tensor] = None
        if packed_inputs is not None:
            if input_ids is None:
                raise ValueError("`input_ids` must be provided when using `packed_inputs`.")
            inputs_embeds, precomputed_modality = self.embed_packed_inputs(input_ids, packed_inputs)
            precomputed_position_ids = packed_inputs.get("position_ids")
            if precomputed_position_ids is not None:
                precomputed_position_ids = precomputed_position_ids.to(inputs_embeds.device)
            tensor_stream = None
        elif input_ids is not None:
            inputs_embeds = self.text_model.embed_tokens(input_ids)
        elif inputs_embeds is None:
            raise ValueError("You have to specify either tensor_stream, packed_inputs, input_ids or inputs_embeds")

        batch_size, seq_len = inputs_embeds.shape[:2]

        # Ensure cache exists when requested
        if use_cache and past_key_values is None:
            cache_config = self.config.get_text_config() if hasattr(self.config, "get_text_config") else self.config
            past_key_values = DynamicCache(config=cache_config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(past_seen_tokens, past_seen_tokens + seq_len, device=inputs_embeds.device)

        if attention_mask is None:
            attention_mask = torch.ones((batch_size, seq_len), device=inputs_embeds.device, dtype=torch.long)

        if modality_tensor is None and precomputed_modality is not None:
            modality_tensor = precomputed_modality

        position_arg = position_ids if position_ids is not None else precomputed_position_ids

        position_ids, modality_tensor, decoder_position_ids, cos, sin = self._prepare_position_and_modality(
            position_ids=position_arg,
            modality_tensor=modality_tensor,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
        )

        # Prepare attention mask
        if not isinstance(attention_mask, dict):
            attention_mask = create_masks_for_generate(
                config=self.config,
                input_embeds=inputs_embeds,
                attention_mask=attention_mask,
                cache_position=cache_position,
                past_key_values=past_key_values,
                position_ids=decoder_position_ids,
            )

        is_attention_mask_dict = isinstance(attention_mask, dict)

        # Initialize hidden states
        hidden_states = inputs_embeds
        all_attentions = [] if output_attentions else None

        for decoder_layer in self.text_model.layers:
            layer_attention_mask = (
                attention_mask[decoder_layer.attention_type] if is_attention_mask_dict else attention_mask
            )
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=layer_attention_mask,
                position_ids=decoder_position_ids,
                past_key_values=past_key_values,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=(cos, sin),
                output_attentions=output_attentions,
                **kwargs,
            )

            layer_outputs_is_tuple = isinstance(layer_outputs, tuple)
            hidden_states = layer_outputs[0] if layer_outputs_is_tuple else layer_outputs
            if output_attentions and layer_outputs_is_tuple:
                all_attentions.append(layer_outputs[1])

        # Final layer norm
        hidden_states = self.text_model.norm(hidden_states)

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
            hidden_states=(hidden_states,),
            attentions=tuple(all_attentions) if output_attentions else None,
        )


@use_kernel_forward_from_hub("RMSNorm")
class IsaacRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps: float = 1e-6) -> None:
        """
        IsaacRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


@use_kernel_func_from_hub("rotary_pos_emb")
def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


@use_kernelized_func(apply_rotary_pos_emb)
class IsaacAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: IsaacConfig, layer_idx: int):
        super().__init__()
        self.layer_type = config.layer_types[layer_idx] if hasattr(config, "layer_types") else None
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.q_norm = IsaacRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
        self.k_norm = IsaacRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape
        self.sliding_window = config.sliding_window if self.layer_type == "sliding_attention" else None

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_values is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,  # diff with Llama
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class IsaacDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: IsaacConfig, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = IsaacAttention(config=config, layer_idx=layer_idx)

        self.mlp = IsaacMLP(config)
        self.input_layernorm = IsaacRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = IsaacRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attention_type = config.layer_types[layer_idx]

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> torch.Tensor:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        # Self Attention
        hidden_states, _ = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        return hidden_states


@auto_docstring
class IsaacPreTrainedModel(PreTrainedModel):
    config: IsaacConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["IsaacDecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": IsaacDecoderLayer,
        "attentions": IsaacAttention,
    }


@auto_docstring
class IsaacForConditionalGeneration(IsaacPreTrainedModel, GenerationMixin):
    """Isaac multimodal model for conditional generation."""

    _tied_weights_keys = {"lm_head.weight": "model.text_model.embed_tokens.weight"}
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    config_class = IsaacConfig
    _can_compile_fullgraph = False
    all_tied_weights_keys: dict[str, str] = {"lm_head.weight": "model.text_model.embed_tokens.weight"}

    def __init__(self, config: IsaacConfig):
        super().__init__(config)
        self.model = IsaacModel(config)  # Use our custom model
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        # Tracks rotary position offsets computed during a full forward pass so decode steps can reuse them.
        self.rope_deltas = None

        # Initialize weights and apply final processing
        self.post_init()

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        tensor_stream: Optional[TensorStream] = None,
        packed_inputs: Optional[dict[str, torch.Tensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[list[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple | CausalLMOutputWithPast:
        r"""
        Forward pass for conditional generation supporting both standard inputs, TensorStream, and packed_inputs.

        tensor_stream (`TensorStream`, *optional*):
            Packed multimodal stream (text, vision, audio tokens) that already encodes spatial metadata. When provided,
            the model derives embeddings, modality masks, and 3D rotary coordinates directly from the stream instead of
            `input_ids`.
        packed_inputs (`dict`, *optional*):
            Plain tensors extracted from a TensorStream; requires `input_ids` for text tokens.
        """

        output_attentions = kwargs.pop("output_attentions", None)

        # Don't compute embeddings here - let the inner model handle it
        if tensor_stream is not None and packed_inputs is not None:
            tensor_stream = None  # prefer packed_inputs when both are present
        if tensor_stream is not None:
            input_ids = None
        if packed_inputs is not None and input_ids is None and inputs_embeds is None:
            raise ValueError("`input_ids` must be provided when using `packed_inputs`.")
        if input_ids is None and inputs_embeds is None and tensor_stream is None and packed_inputs is None:
            raise ValueError("Either input_ids, inputs_embeds, tensor_stream, or packed_inputs must be provided.")

        # Record rope deltas on prefill when TensorStream is provided; leave position_ids building to IsaacModel.
        if position_ids is None and tensor_stream is not None:
            position_ids, self.rope_deltas = self.get_rope_index(input_ids, tensor_stream, attention_mask)
        elif position_ids is None and cache_position is not None and self.rope_deltas is not None:
            # Decode continuation after TensorStream prefill: advance positions using cached rope offsets.
            if input_ids is not None:
                base_position_ids = compute_position_ids_input_ids(input_ids)
            else:
                if inputs_embeds is None:
                    raise ValueError("inputs_embeds must be provided when input_ids is None during decode")
                batch_size, seq_len = inputs_embeds.shape[:2]
                dummy_ids = torch.zeros((batch_size, seq_len), device=inputs_embeds.device, dtype=torch.long)
                base_position_ids = compute_position_ids_input_ids(dummy_ids)

            rope_delta = (cache_position[0] + self.rope_deltas).to(base_position_ids.device)
            if not isinstance(rope_delta, int):
                rope_delta = rope_delta.repeat_interleave(base_position_ids.shape[0] // rope_delta.shape[0], dim=0)
            position_ids = base_position_ids.add(rope_delta)

        outputs = self.model(
            input_ids=input_ids,
            tensor_stream=tensor_stream,
            packed_inputs=packed_inputs,
            attention_mask=attention_mask,
            position_ids=position_ids,
            modality_tensor=None,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = outputs[0]
        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions if output_attentions else None,
        )

    def set_input_embeddings(self, value: nn.Module) -> None:
        self.model.set_input_embeddings(value)
        vocab_size = getattr(value, "num_embeddings", None)
        if vocab_size is not None:
            self.config.vocab_size = vocab_size
            self.model.config.vocab_size = vocab_size
            if hasattr(self.model, "text_model"):
                self.model.text_model.config.vocab_size = vocab_size
            if self.lm_head.weight.shape[0] != vocab_size:
                self.lm_head = nn.Linear(self.config.hidden_size, vocab_size, bias=False)
            if hasattr(self.model, "embed_tokens"):
                self.lm_head.weight = self.model.text_model.embed_tokens.weight

    def get_rope_index(
        self,
        input_ids: Optional[torch.Tensor],
        tensor_stream: Optional[TensorStream],
        attention_mask: Optional[torch.Tensor],
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Compute MRoPE position ids from a TensorStream (or 1D fallback).

        Returns (position_ids, rope_deltas). position_ids is (B,L,3) for MRoPE.
        rope_deltas is (B,1) used to advance positions in decode.
        """
        # tensor_stream present: compute 3D coords
        if tensor_stream is None and input_ids is None:
            raise ValueError("`tensor_stream` or `input_ids` must be provided to compute rope indices")

        if tensor_stream is not None:
            pos_3d = compute_mrope_pos_tensor(tensor_stream)  # (B,L,3)
        else:
            pos_3d = compute_position_ids_input_ids(input_ids)
        B, L, _ = pos_3d.shape

        # Max position per batch across the 3 planes and sequence dimension: (B,)
        m_per_batch = pos_3d.amax(dim=(1, 2))

        # Sequence lengths per batch: (B,)
        if attention_mask is None:
            seq_lens = torch.full_like(m_per_batch, L)
        else:
            seq_lens = attention_mask.eq(1).sum(dim=-1).to(dtype=m_per_batch.dtype, device=m_per_batch.device)

        rope_deltas = (m_per_batch + 1 - seq_lens).to(dtype=pos_3d.dtype).unsqueeze(1)
        return pos_3d, rope_deltas

    def prepare_inputs_for_generation(
        self,
        input_ids: torch.LongTensor,
        past_key_values: Optional[list[torch.FloatTensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        tensor_stream: Optional[TensorStream] = None,
        packed_inputs: Optional[dict[str, torch.Tensor]] = None,
        cache_position: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        use_cache: bool = True,
        **kwargs,
    ) -> dict[str, Any]:
        """
        Prepare inputs for generation, handling TensorStream and packed_inputs inputs properly.
        """
        if tensor_stream is not None and packed_inputs is not None:
            raise ValueError("Provide only one of `tensor_stream` or `packed_inputs` during generation prep.")

        if cache_position is None:
            seq_length = None
            device = None
            if input_ids is not None:
                seq_length = input_ids.shape[1]
                device = input_ids.device
            elif inputs_embeds is not None:
                seq_length = inputs_embeds.shape[1]
                device = inputs_embeds.device
            elif tensor_stream is not None:
                _, seq_length = tensor_stream.shape
                device = tensor_stream.device
            if seq_length is not None:
                # prepare_inputs_for_generation may be invoked outside `generate`, so synthesize the
                # same cache positions that GenerationMixin would have created during prefill.
                cache_position = torch.arange(seq_length, dtype=torch.long, device=device)

        # Call parent preparation
        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            position_ids=position_ids,
            use_cache=use_cache,
            **kwargs,
        )

        cache_position = model_inputs.get("cache_position", cache_position)

        # Handle TensorStream/packed_inputs only for the prefill step
        first_step = cache_position is None or cache_position[0] == 0
        if tensor_stream is not None and first_step:
            model_inputs["tensor_stream"] = tensor_stream
            model_inputs["position_ids"] = None
        else:
            model_inputs["tensor_stream"] = None

        if packed_inputs is not None and first_step:
            model_inputs["packed_inputs"] = packed_inputs
            model_inputs["position_ids"] = None
            model_inputs["tensor_stream"] = None
        else:
            model_inputs["packed_inputs"] = None

        # TensorStream decode path: preserve rotary offsets from prefill; let forward rebuild positions
        if tensor_stream is not None and not first_step and self.rope_deltas is not None:
            model_inputs["position_ids"] = None
            return model_inputs

        return model_inputs

    @classmethod
    def can_generate(cls) -> bool:
        return True


__all__ = ["IsaacModel", "IsaacPreTrainedModel", "IsaacForConditionalGeneration"]
