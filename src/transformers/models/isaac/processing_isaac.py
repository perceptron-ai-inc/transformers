#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/isaac/modular_isaac.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_isaac.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
# coding=utf-8
# Copyright 2025 Perceptron, Inc and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Any, Optional, Union

from ...feature_extraction_utils import BatchFeature
from ...processing_utils import ProcessorMixin
from ...utils import TensorType
from ...utils.import_utils import is_torch_available, is_vision_available
from .configuration_isaac import IsaacConfig
from .modeling_isaac import ModalityType


if is_torch_available():
    import torch


if is_vision_available():
    from PIL.Image import Image
else:
    Image = None


# ============================================================================
# Processor
# ============================================================================


class IsaacProcessor(ProcessorMixin):
    attributes = ["image_processor", "tokenizer"]
    image_processor_class = ("IsaacImageProcessorFast",)
    tokenizer_class = ("Qwen2Tokenizer",)

    def __init__(
        self,
        image_processor,
        tokenizer,
        *,
        vision_token: str = "<image>",
        max_sequence_length: int = 16384,
        rescale_factor: Optional[float] = None,
        config: Optional[Union["IsaacConfig", dict]] = None,
    ) -> None:
        if isinstance(config, dict):
            config = IsaacConfig(**config)

        if config is not None:
            vision_token = config.vision_token
            max_sequence_length = config.max_sequence_length
            rescale_factor = config.vision_rescale_factor

        resolved_rescale_factor = float(rescale_factor) if rescale_factor is not None else float(1 / 255)
        if config is not None:
            config.vision_rescale_factor = resolved_rescale_factor

        self.image_processor = image_processor
        super().__init__(image_processor, tokenizer)

        self.current_processor = self.image_processor
        self.config = config
        self.chat_template = getattr(self.tokenizer, "chat_template", None)
        self.vision_token = vision_token
        self.max_sequence_length = max_sequence_length

    def _pack_single(self, text: str, images: Optional[list["Image"]]) -> dict[str, Optional["torch.Tensor"]]:
        # Parse by vision_token; interleave text segments and image segments.
        segs = text.split(self.vision_token)
        n_img = len(segs) - 1
        if n_img and (images is None or len(images) != n_img):
            raise ValueError(
                f"Number of {self.vision_token} tokens ({n_img}) must match images ({0 if images is None else len(images)})."
            )

        items: list[dict[str, Any]] = []
        total = 0

        for i, s in enumerate(segs):
            if s:
                tok = self.tokenizer.encode(s, add_special_tokens=False, return_tensors="pt").squeeze(0).to(torch.long)
                L = int(tok.numel())
                items.append({"t": "text", "L": L, "tok": tok})
                total += L

            if i < n_img:
                feat = self.image_processor(images=images[i], return_tensors=TensorType.PYTORCH)
                patches = feat["patches"][0].reshape(-1, feat["patches"].shape[-1])

                v = feat["virtual_pixel_size"][0].to(torch.long).tolist()
                r = feat["real_pixel_size"][0].to(torch.long).tolist()
                dims = tuple((v + [1, 1, 1])[:3])  # (T,H,W) in virtual space
                L = int(dims[0] * dims[1] * dims[2])

                items.append({"t": "image", "L": L, "dims": dims, "patches": patches, "grid": (int(r[1]), int(r[2]))})
                total += L

        # Tail crop window.
        start = max(0, total - self.max_sequence_length)
        end = total

        fill_value = 151643
        base_device: Optional[torch.device] = None
        pos, mod, ids = [], [], []
        vpatches, grids, offs, lens = [], [], [], []

        cursor = 0
        t0 = 0

        for it in items:
            L = int(it["L"])
            a = max(start, cursor)
            b = min(end, cursor + L)
            keep = b > a

            if keep and base_device is None:
                base_device = it["patches"].device if it["t"] == "image" else it["tok"].device

            if keep:
                lo = int(a - cursor)
                hi = int(b - cursor)
                k = torch.arange(lo, hi, device=base_device, dtype=torch.long)
                n = hi - lo

                if it["t"] == "text":
                    t = k + t0
                    z = torch.zeros_like(t)
                    pos.append(torch.stack((t, z, z), -1))
                    mod.append(torch.full((n,), ModalityType.text.value, device=base_device, dtype=torch.long))
                    ids.append(it["tok"].to(base_device)[lo:hi])
                    t0 += L
                else:
                    T, H, W = it["dims"]
                    hw = H * W
                    t = (k // hw) + t0
                    rem = k % hw
                    h = rem // W
                    w = rem % W
                    pos.append(torch.stack((t, h, w), -1))
                    mod.append(torch.full((n,), ModalityType.image.value, device=base_device, dtype=torch.long))
                    ids.append(torch.full((n,), fill_value, device=base_device, dtype=torch.long))

                    vpatches.append(it["patches"].to(base_device))  # full patches; slice later via offsets/lengths
                    grids.append(it["grid"])
                    offs.append(lo)
                    lens.append(n)

                    t0 += int(T)
            else:
                t0 += L if it["t"] == "text" else int(it["dims"][0])

            cursor += L

        modality_tensor = (
            torch.cat(mod, 0).unsqueeze(0) if mod else torch.zeros((1, 0), device=base_device, dtype=torch.long)
        )
        position_ids = (
            torch.cat(pos, 0).unsqueeze(0) if pos else torch.zeros((1, 0, 3), device=base_device, dtype=torch.long)
        )
        input_ids = (
            torch.cat(ids, 0).unsqueeze(0) if ids else torch.zeros((1, 0), device=base_device, dtype=torch.long)
        )

        if vpatches:
            vision_patches = torch.cat(vpatches, 0)
            vision_token_grids = torch.tensor(grids, device=base_device, dtype=torch.long)
            vision_token_offsets = torch.tensor(offs, device=base_device, dtype=torch.long)
            vision_token_lengths = torch.tensor(lens, device=base_device, dtype=torch.long)
        else:
            vision_patches = vision_token_grids = vision_token_offsets = vision_token_lengths = None

        return {
            "input_ids": input_ids,
            "vision_patches": vision_patches,
            "vision_token_grids": vision_token_grids,
            "vision_token_offsets": vision_token_offsets,
            "vision_token_lengths": vision_token_lengths,
            "modality_tensor": modality_tensor,
            "position_ids": position_ids,
        }

    def __call__(
        self,
        text: Union[str, list[str]],
        images: Optional[Union["Image", list["Image"]]] = None,
        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,
        **kwargs,
    ) -> BatchFeature:
        texts = [text] if isinstance(text, str) else text
        if len(texts) != 1:
            raise ValueError("IsaacProcessor currently supports batch_size=1")

        images_list = None
        if images is not None:
            images_list = [images] if isinstance(images, Image) else images
            n_tok = texts[0].count(self.vision_token)
            if n_tok != len(images_list):
                raise ValueError(
                    f"Number of {self.vision_token} tokens in text ({n_tok}) must match number of images ({len(images_list)})"
                )

        packed = self._pack_single(texts[0], images_list)
        input_ids = packed.pop("input_ids")
        return BatchFeature(data={"input_ids": input_ids, "packed_inputs": packed})


__all__ = ["IsaacProcessor"]
