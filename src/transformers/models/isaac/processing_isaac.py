#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/isaac/modular_isaac.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_isaac.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 Perceptron, Inc and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Any, Optional, Union

from ...feature_extraction_utils import BatchFeature
from ...processing_utils import ProcessorMixin
from ...utils import TensorType
from ...utils.import_utils import is_torch_available, is_vision_available
from .configuration_isaac import IsaacConfig
from .modeling_isaac import ModalityType


if is_torch_available():
    import torch


if is_vision_available():
    from PIL.Image import Image
else:
    Image = None


def tensor_stream_token_view(
    text_token_ids: torch.Tensor,
    modality_tensor: torch.Tensor,
    *,
    fill_value: int = 0,
) -> torch.Tensor:
    """
    Compose a (batch, seq_len) token tensor using text token ids and a modality map.
    Non-text positions are filled with ``fill_value``.
    """
    if text_token_ids.dim() == 1:
        text_token_ids = text_token_ids.unsqueeze(0)

    B, L = modality_tensor.shape
    if text_token_ids.size(0) == 1 and B > 1:
        text_token_ids = text_token_ids.expand(B, -1)

    text_mask = modality_tensor.eq(ModalityType.text.value)
    # rank of each text position within its row: 0..need-1
    rank = text_mask.cumsum(dim=1) - 1  # -1 where mask is False

    out = modality_tensor.new_full((B, L), fill_value, dtype=torch.long)
    gathered = text_token_ids.gather(1, rank.clamp_min(0))
    out[text_mask] = gathered[text_mask]
    return out


# ============================================================================
# Processor
# ============================================================================


class IsaacProcessor(ProcessorMixin):
    attributes = ["image_processor", "tokenizer"]
    image_processor_class = ("IsaacImageProcessorFast",)
    tokenizer_class = ("Qwen2Tokenizer",)

    def __init__(
        self,
        image_processor,
        tokenizer,
        *,
        vision_token: str = "<image>",
        max_sequence_length: int = 16384,
        rescale_factor: Optional[float] = None,
        config: Optional[Union["IsaacConfig", dict]] = None,
    ) -> None:
        if isinstance(config, dict):
            config = IsaacConfig(**config)

        if config is not None:
            vision_token = config.vision_token
            max_sequence_length = config.max_sequence_length
            rescale_factor = config.vision_rescale_factor

        resolved_rescale_factor = float(rescale_factor) if rescale_factor is not None else float(1 / 255)
        if config is not None:
            config.vision_rescale_factor = resolved_rescale_factor

        self.image_processor = image_processor
        super().__init__(image_processor, tokenizer)

        self.current_processor = self.image_processor
        self.config = config
        self.chat_template = getattr(self.tokenizer, "chat_template", None)
        self.vision_token = vision_token
        self.max_sequence_length = max_sequence_length

    def _pack_single(self, text: str, images: Optional[list["Image"]]) -> dict[str, Optional["torch.Tensor"]]:
        # Parse by vision_token; interleave text segments and image segments.
        segs = text.split(self.vision_token)
        n_img = len(segs) - 1
        if n_img and (images is None or len(images) != n_img):
            raise ValueError(
                f"Number of {self.vision_token} tokens ({n_img}) must match images ({0 if images is None else len(images)})."
            )

        items: list[dict[str, Any]] = []
        total = 0

        for i, s in enumerate(segs):
            if s:
                tok = self.tokenizer.encode(s, add_special_tokens=False, return_tensors="pt").squeeze(0).to(torch.long)
                L = int(tok.numel())
                items.append({"t": "text", "L": L, "tok": tok})
                total += L

            if i < n_img:
                feat = self.image_processor(images=images[i], return_tensors=TensorType.PYTORCH)
                patches = feat["patches"][0].reshape(-1, feat["patches"].shape[-1])

                v = feat["virtual_pixel_size"][0].to(torch.long).tolist()
                r = feat["real_pixel_size"][0].to(torch.long).tolist()
                dims = tuple((v + [1, 1, 1])[:3])  # (T,H,W) in virtual space
                L = int(dims[0] * dims[1] * dims[2])

                items.append({"t": "image", "L": L, "dims": dims, "patches": patches, "grid": (int(r[1]), int(r[2]))})
                total += L

        # Tail crop window.
        start = max(0, total - self.max_sequence_length)
        end = total

        base_device: Optional[torch.device] = None
        pos, mod, txt = [], [], []
        vpatches, grids, offs, lens = [], [], [], []

        cursor = 0
        t0 = 0

        for it in items:
            L = int(it["L"])
            a = max(start, cursor)
            b = min(end, cursor + L)
            keep = b > a

            if keep and base_device is None:
                base_device = it["patches"].device if it["t"] == "image" else it["tok"].device

            if keep:
                lo = int(a - cursor)
                hi = int(b - cursor)
                k = torch.arange(lo, hi, device=base_device, dtype=torch.long)
                n = hi - lo

                if it["t"] == "text":
                    t = k + t0
                    z = torch.zeros_like(t)
                    pos.append(torch.stack((t, z, z), -1))
                    mod.append(torch.full((n,), ModalityType.text.value, device=base_device, dtype=torch.long))
                    txt.append(it["tok"].to(base_device)[lo:hi])
                    t0 += L
                else:
                    T, H, W = it["dims"]
                    hw = H * W
                    t = (k // hw) + t0
                    rem = k % hw
                    h = rem // W
                    w = rem % W
                    pos.append(torch.stack((t, h, w), -1))
                    mod.append(torch.full((n,), ModalityType.image.value, device=base_device, dtype=torch.long))

                    vpatches.append(it["patches"].to(base_device))  # full patches; slice later via offsets/lengths
                    grids.append(it["grid"])
                    offs.append(lo)
                    lens.append(n)

                    t0 += int(T)
            else:
                t0 += L if it["t"] == "text" else int(it["dims"][0])

            cursor += L

        if base_device is None:
            base_device = torch.device("cpu")

        modality_tensor = (
            torch.cat(mod, 0).unsqueeze(0) if mod else torch.zeros((1, 0), device=base_device, dtype=torch.long)
        )
        position_ids = (
            torch.cat(pos, 0).unsqueeze(0) if pos else torch.zeros((1, 0, 3), device=base_device, dtype=torch.long)
        )
        text_token_ids = (
            torch.cat(txt, 0).unsqueeze(0) if txt else torch.zeros((1, 0), device=base_device, dtype=torch.long)
        )

        if vpatches:
            vision_patches = torch.cat(vpatches, 0)
            vision_token_grids = torch.tensor(grids, device=base_device, dtype=torch.long)
            vision_token_offsets = torch.tensor(offs, device=base_device, dtype=torch.long)
            vision_token_lengths = torch.tensor(lens, device=base_device, dtype=torch.long)
        else:
            vision_patches = vision_token_grids = vision_token_offsets = vision_token_lengths = None

        return {
            "vision_patches": vision_patches,
            "vision_token_grids": vision_token_grids,
            "vision_token_offsets": vision_token_offsets,
            "vision_token_lengths": vision_token_lengths,
            "modality_tensor": modality_tensor,
            "position_ids": position_ids,
            "text_token_ids": text_token_ids,
        }

    def __call__(
        self,
        text: Union[str, list[str]],
        images: Optional[Union["Image", list["Image"]]] = None,
        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,
        **kwargs,
    ) -> "BatchFeature":
        texts = [text] if isinstance(text, str) else text
        if len(texts) != 1:
            raise ValueError("IsaacProcessor currently supports batch_size=1")

        images_list = None
        if images is not None:
            images_list = [images] if isinstance(images, Image) else images
            n_tok = texts[0].count(self.vision_token)
            if n_tok != len(images_list):
                raise ValueError(
                    f"Number of {self.vision_token} tokens in text ({n_tok}) must match number of images ({len(images_list)})"
                )

        packed = self._pack_single(texts[0], images_list)

        fill_value = getattr(self.tokenizer, "pad_token_id", 0)
        if fill_value is None or fill_value < 0:
            fill_value = 0

        tokens = tensor_stream_token_view(packed["text_token_ids"], packed["modality_tensor"], fill_value=fill_value)
        input_ids = (
            torch.as_tensor(tokens, dtype=torch.long) if return_tensors in (TensorType.PYTORCH, "pt") else tokens
        )

        return BatchFeature(data={"input_ids": input_ids, "packed_inputs": packed})


__all__ = ["IsaacProcessor"]
