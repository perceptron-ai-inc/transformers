#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/isaac/modular_isaac.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_isaac.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 Perceptron, Inc and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import itertools
import math
import re
from typing import Any, Optional, Union

from ...feature_extraction_utils import BatchFeature
from ...processing_utils import ProcessorMixin
from ...utils import TensorType
from ...utils.import_utils import is_torch_available, is_vision_available
from .configuration_isaac import IsaacConfig
from .modeling_isaac import ModalityType


if is_torch_available():
    import torch


if is_vision_available():
    from PIL.Image import Image
else:
    Image = None


def tensor_stream_token_view(
    text_token_ids: torch.Tensor,
    modality_tensor: torch.Tensor,
    *,
    fill_value: int = 0,
) -> torch.Tensor:
    """
    Compose a (batch, seq_len) token tensor using text token ids and a modality map.
    Non-text positions are filled with ``fill_value``.
    """
    if modality_tensor.dim() != 2:
        raise ValueError("`modality_tensor` must be 2D (batch, seq_len).")

    if text_token_ids.dim() == 1:
        text_token_ids = text_token_ids.unsqueeze(0)
    if text_token_ids.dim() != 2:
        raise ValueError("`text_token_ids` must be 1D or 2D.")

    batch_size, seq_len = modality_tensor.shape
    token_batch = text_token_ids.shape[0]
    if token_batch not in {1, batch_size}:
        raise ValueError("`text_token_ids` batch dimension must match `modality_tensor` or be 1.")

    if token_batch == 1 and batch_size > 1:
        text_token_ids = text_token_ids.expand(batch_size, -1)

    text_mask = modality_tensor == ModalityType.text.value
    text_counts = text_mask.sum(dim=1)
    max_text_needed = int(text_counts.max().item()) if text_counts.numel() else 0

    if text_token_ids.size(1) < max_text_needed:
        raise ValueError("`text_token_ids` does not have enough tokens for the provided modalities.")

    tokens = modality_tensor.new_full(
        (batch_size, seq_len), fill_value, dtype=torch.long, device=modality_tensor.device
    )
    for batch_idx in range(batch_size):
        needed = int(text_counts[batch_idx].item())
        if needed == 0:
            continue
        tokens[batch_idx, text_mask[batch_idx]] = text_token_ids[batch_idx, :needed].to(
            device=tokens.device, dtype=torch.long
        )

    return tokens


# ============================================================================
# Processor
# ============================================================================


class IsaacProcessor(ProcessorMixin):
    attributes = ["image_processor", "tokenizer"]
    image_processor_class = ("IsaacImageProcessorFast",)
    tokenizer_class = ("Qwen2Tokenizer",)

    def __init__(
        self,
        image_processor,
        tokenizer,
        *,
        vision_token: str = "<image>",
        max_sequence_length: int = 16384,
        rescale_factor: Optional[float] = None,
        config: Optional[Union[IsaacConfig, dict]] = None,
    ) -> None:
        if tokenizer is None:
            raise ValueError("`tokenizer` must be provided to initialize IsaacProcessor.")

        if isinstance(config, dict):
            config = IsaacConfig(**config)

        if config is not None:
            max_sequence_length = config.max_sequence_length
            vision_token = config.vision_token
            rescale_factor = config.vision_rescale_factor

        resolved_rescale_factor = float(rescale_factor) if rescale_factor is not None else float(1 / 255)

        if config is not None:
            config.vision_rescale_factor = resolved_rescale_factor

        self.image_processor = image_processor

        super().__init__(image_processor, tokenizer)
        self.current_processor = self.image_processor
        self.config = config

        self.chat_template = getattr(self.tokenizer, "chat_template", None)
        self.vision_token = vision_token
        self.max_sequence_length = max_sequence_length

    def _segment_inputs(
        self,
        text: str,
        images: Optional[list[Image]],
    ) -> list[dict[str, Any]]:
        pattern = re.escape(self.vision_token)
        parts = re.split(f"({pattern})", text)

        segments: list[dict[str, Any]] = []
        image_idx = 0
        for part in parts:
            if part == self.vision_token:
                if images is None or image_idx >= len(images):
                    raise ValueError("Encountered vision token without a corresponding image.")

                features = self.image_processor(
                    images=images[image_idx],
                    return_tensors=TensorType.PYTORCH,
                )

                patches = features["patches"][0]
                virtual_dims = [int(v) for v in features["virtual_pixel_size"][0].tolist()]
                real_dims = [int(v) for v in features["real_pixel_size"][0].tolist()]

                segments.append(
                    {
                        "type": "image",
                        "seq_length": int(math.prod(virtual_dims)),
                        "patches": patches.reshape(-1, patches.shape[-1]),
                        "virtual_dims": virtual_dims,
                        "real_dims": real_dims,
                    }
                )
                image_idx += 1
            elif part:
                tokens = self.tokenizer.encode(part, add_special_tokens=False, return_tensors="pt").squeeze(0)
                tokens = tokens.to(dtype=torch.long)
                segments.append(
                    {
                        "type": "text",
                        "length": int(tokens.shape[0]),
                        "tokens": tokens,
                    }
                )

        return segments

    def _build_packed_inputs_from_segments(self, segments: list[dict[str, Any]]) -> dict[str, Optional[torch.Tensor]]:
        total_len = 0
        for seg in segments:
            if seg["type"] == "text":
                total_len += int(seg["length"])
            else:
                total_len += int(seg.get("seq_length", seg["patches"].shape[0]))

        start_keep = max(0, total_len - self.max_sequence_length)
        end_keep = total_len

        modality_values: list[int] = []
        coords: list[tuple[int, int, int]] = []
        text_chunks: list[torch.Tensor] = []
        vision_chunks: list[torch.Tensor] = []
        token_grids: list[tuple[int, int]] = []
        token_offsets: list[int] = []
        token_lengths: list[int] = []

        token_cursor = 0
        time_offset = 0

        for seg in segments:
            if seg["type"] == "text":
                seq_len = int(seg["length"])
                dims = [seq_len, 1, 1]
                data = seg["tokens"]
            else:
                dims = list(seg["virtual_dims"])
                if len(dims) < 3:
                    dims = dims + [1] * (3 - len(dims))
                seq_len = int(seg.get("seq_length", math.prod(dims)))
                data = seg["patches"]

            seg_len = seq_len
            seg_start = token_cursor
            seg_end = seg_start + seg_len

            overlap_start = max(start_keep, seg_start)
            overlap_end = min(end_keep, seg_end)
            overlap_len = overlap_end - overlap_start

            first_dim = range(time_offset, time_offset + dims[0])
            other_dims = [range(d) for d in dims[1:]]
            full_coords = list(itertools.product(first_dim, *other_dims))

            if overlap_len > 0:
                slice_start = overlap_start - seg_start
                slice_end = slice_start + overlap_len
                coords.extend(full_coords[slice_start:slice_end])

                modality_value = ModalityType.text.value if seg["type"] == "text" else ModalityType.image.value
                modality_values.extend([modality_value] * overlap_len)

                if seg["type"] == "text":
                    text_1d = data.view(-1)
                    text_chunks.append(text_1d[slice_start:slice_end])
                else:
                    vision_chunks.append(data)
                    real_dims = seg["real_dims"]
                    token_grids.append((int(real_dims[1]), int(real_dims[2])))
                    token_offsets.append(int(slice_start))
                    token_lengths.append(int(overlap_len))

            token_cursor = seg_end
            time_offset += dims[0]

        if vision_chunks:
            base_device = vision_chunks[0].device
        elif text_chunks:
            base_device = text_chunks[0].device
        else:
            base_device = torch.device("cpu")

        if modality_values:
            modality_tensor = torch.tensor(modality_values, dtype=torch.long, device=base_device).unsqueeze(0)
        else:
            modality_tensor = torch.zeros((1, 0), dtype=torch.long, device=base_device)

        if coords:
            position_ids = torch.tensor(coords, dtype=torch.long, device=base_device).reshape(1, len(coords), 3)
        else:
            position_ids = torch.zeros((1, 0, 3), dtype=torch.long, device=base_device)

        if text_chunks:
            text_concat = torch.cat(text_chunks, dim=0).to(device=base_device, dtype=torch.long)
        else:
            text_concat = torch.zeros((0,), dtype=torch.long, device=base_device)

        text_token_ids = torch.zeros((1, text_concat.numel()), dtype=torch.long, device=base_device)
        if text_concat.numel():
            text_token_ids[0, : text_concat.numel()] = text_concat

        if vision_chunks:
            vision_patches = torch.cat(vision_chunks, dim=0)
            vision_token_grids = torch.tensor(token_grids, dtype=torch.long, device=vision_patches.device)
            vision_token_offsets = torch.tensor(token_offsets, dtype=torch.long, device=vision_patches.device)
            vision_token_lengths = torch.tensor(token_lengths, dtype=torch.long, device=vision_patches.device)
        else:
            vision_patches = None
            vision_token_grids = None
            vision_token_offsets = None
            vision_token_lengths = None

        return {
            "vision_patches": vision_patches,
            "vision_token_grids": vision_token_grids,
            "vision_token_offsets": vision_token_offsets,
            "vision_token_lengths": vision_token_lengths,
            "modality_tensor": modality_tensor,
            "position_ids": position_ids,
            "text_token_ids": text_token_ids,
        }

    def __call__(
        self,
        text: Union[str, list[str]],
        images: Optional[Union[Image, list[Image]]] = None,
        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,
        **kwargs,
    ) -> BatchFeature:
        """
        Process text and images into packed tensors (and optionally TensorStream for compatibility).
        Args:
            text: Input text or list of texts with vision tokens
            images: PIL image or list of images (optional)
            return_tensors: Format for output tensors
            return_tensor_stream: When True, include the legacy TensorStream in the output.

        Returns:
            BatchFeature with input_ids, packed_inputs, and optionally tensor_stream. packed_inputs bundles the
            plain tensors (vision_patches, vision_token_grids, modality_tensor, position_ids, vision_token_offsets,
            vision_token_lengths) needed to describe the multimodal sequence without TensorStream.
        """
        # Normalize inputs to lists
        if isinstance(text, str):
            texts = [text]
        else:
            texts = text

        if images is not None:
            if isinstance(images, Image):
                images_list = [images]
            else:
                images_list = images
        else:
            images_list = None

        if len(texts) != 1:
            raise ValueError("IsaacProcessor currently supports batch_size=1")
        if images_list is not None:
            # Count vision tokens in text to validate image count
            vision_token_count = texts[0].count(self.vision_token)
            if vision_token_count != len(images_list):
                raise ValueError(
                    f"Number of {self.vision_token} tokens in text ({vision_token_count}) "
                    f"must match number of images ({len(images_list)})"
                )

        segments = self._segment_inputs(texts[0], images_list)
        packed_inputs = self._build_packed_inputs_from_segments(segments)

        text_token_ids = packed_inputs.get("text_token_ids")
        if text_token_ids is None:
            raise ValueError("`text_token_ids` is required to build input ids from packed inputs.")

        modality_tensor = packed_inputs.get("modality_tensor")
        if modality_tensor is None:
            raise ValueError("`modality_tensor` is required to build input ids from packed inputs.")

        fill_value = getattr(self.tokenizer, "pad_token_id", None)
        if fill_value is None or fill_value < 0:
            fill_value = 0

        tokens = tensor_stream_token_view(text_token_ids, modality_tensor, fill_value=fill_value)
        if return_tensors in (TensorType.PYTORCH, "pt"):
            input_ids = torch.as_tensor(tokens, dtype=torch.long)
        else:
            input_ids = tokens

        data = {
            "input_ids": input_ids,
            "packed_inputs": packed_inputs,
        }

        return BatchFeature(data=data)


__all__ = ["IsaacProcessor"]
