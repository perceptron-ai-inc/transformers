#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/isaac/modular_isaac.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_isaac.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 Perceptron, Inc and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
import re
from typing import Any, Optional, Union

from ...feature_extraction_utils import BatchFeature
from ...processing_utils import ProcessorMixin
from ...utils import TensorType
from ...utils.import_utils import is_torch_available, is_vision_available
from .configuration_isaac import IsaacConfig
from .modeling_isaac import ModalityType


if is_torch_available():
    import torch


if is_vision_available():
    from PIL.Image import Image
else:
    Image = None


def tensor_stream_token_view(
    text_token_ids: torch.Tensor,
    modality_tensor: torch.Tensor,
    *,
    fill_value: int = 0,
) -> torch.Tensor:
    """
    Compose a (batch, seq_len) token tensor using text token ids and a modality map.
    Non-text positions are filled with ``fill_value``.
    """
    if text_token_ids.dim() == 1:
        text_token_ids = text_token_ids.unsqueeze(0)

    B, L = modality_tensor.shape
    if text_token_ids.size(0) == 1 and B > 1:
        text_token_ids = text_token_ids.expand(B, -1)

    text_mask = modality_tensor.eq(ModalityType.text.value)
    # rank of each text position within its row: 0..need-1
    rank = text_mask.cumsum(dim=1) - 1  # -1 where mask is False

    out = modality_tensor.new_full((B, L), fill_value, dtype=torch.long)
    gathered = text_token_ids.gather(1, rank.clamp_min(0))
    out[text_mask] = gathered[text_mask]
    return out


# ============================================================================
# Processor
# ============================================================================


class IsaacProcessor(ProcessorMixin):
    attributes = ["image_processor", "tokenizer"]
    image_processor_class = ("IsaacImageProcessorFast",)
    tokenizer_class = ("Qwen2Tokenizer",)

    def __init__(
        self,
        image_processor,
        tokenizer,
        *,
        vision_token: str = "<image>",
        max_sequence_length: int = 16384,
        rescale_factor: Optional[float] = None,
        config: Optional[Union["IsaacConfig", dict]] = None,
    ) -> None:
        if isinstance(config, dict):
            config = IsaacConfig(**config)

        if config is not None:
            vision_token = config.vision_token
            max_sequence_length = config.max_sequence_length
            rescale_factor = config.vision_rescale_factor

        resolved_rescale_factor = float(rescale_factor) if rescale_factor is not None else float(1 / 255)
        if config is not None:
            config.vision_rescale_factor = resolved_rescale_factor

        self.image_processor = image_processor
        super().__init__(image_processor, tokenizer)

        self.current_processor = self.image_processor
        self.config = config
        self.chat_template = getattr(self.tokenizer, "chat_template", None)
        self.vision_token = vision_token
        self.max_sequence_length = max_sequence_length

    def _pack_single(self, text: str, images: Optional[list["Image"]]) -> dict[str, Optional["torch.Tensor"]]:
        # ---- pass 1: tokenize / extract image features + compute total length
        parts = re.split(f"({re.escape(self.vision_token)})", text)

        items: list[dict[str, Any]] = []
        total_len = 0
        img_i = 0

        for part in parts:
            if not part:
                continue

            if part == self.vision_token:
                if images is None or img_i >= len(images):
                    raise ValueError("Encountered vision token without a corresponding image.")

                feat = self.image_processor(images=images[img_i], return_tensors=TensorType.PYTORCH)
                patches = feat["patches"][0].reshape(-1, feat["patches"].shape[-1])
                v = [int(x) for x in feat["virtual_pixel_size"][0].tolist()]
                r = [int(x) for x in feat["real_pixel_size"][0].tolist()]

                # Ensure dims are 3D for position encoding (T,H,W); keep same convention as before.
                dims = (v + [1, 1, 1])[:3]
                seq_len = int(math.prod(dims))

                items.append(
                    {
                        "type": "image",
                        "dims": dims,  # (T,H,W) in "virtual" space
                        "seq_len": seq_len,
                        "patches": patches,  # [seq_len, D] (or [prod(dims), D])
                        "real_dims": r,  # used for token grid
                    }
                )
                total_len += seq_len
                img_i += 1
            else:
                tok = (
                    self.tokenizer.encode(part, add_special_tokens=False, return_tensors="pt")
                    .squeeze(0)
                    .to(torch.long)
                )
                seq_len = int(tok.numel())
                items.append({"type": "text", "seq_len": seq_len, "tokens": tok})
                total_len += seq_len

        # ---- cropping window (keep the tail)
        start_keep = max(0, total_len - self.max_sequence_length)
        end_keep = total_len

        # ---- pass 2: build packed tensors
        modality_vals: list[int] = []
        coords: list[tuple[int, int, int]] = []
        text_chunks: list[torch.Tensor] = []

        vision_chunks: list[torch.Tensor] = []
        token_grids: list[tuple[int, int]] = []
        token_offsets: list[int] = []
        token_lengths: list[int] = []

        cursor = 0  # global token cursor in the concatenated multimodal sequence
        time_offset = 0  # running "T" offset for position_ids

        def append_coords(dims: tuple[int, int, int], base_t: int, lo: int, hi: int) -> None:
            # linear index -> (t, h, w) in row-major order: t-major, then h, then w
            T, H, W = dims
            hw = H * W
            for k in range(lo, hi):
                t = base_t + (k // hw)
                rem = k - (k // hw) * hw
                h = rem // W
                w = rem - h * W
                coords.append((t, h, w))

        for it in items:
            seg_len = int(it["seq_len"])
            seg_start, seg_end = cursor, cursor + seg_len

            overlap_start = max(start_keep, seg_start)
            overlap_end = min(end_keep, seg_end)
            if overlap_end <= overlap_start:
                # still advance offsets
                cursor = seg_end
                time_offset += it["dims"][0] if it["type"] == "image" else seg_len
                continue

            slice_start = int(overlap_start - seg_start)
            slice_end = int(overlap_end - seg_start)
            keep_len = slice_end - slice_start

            if it["type"] == "text":
                # text dims are (seq_len, 1, 1)
                append_coords((seg_len, 1, 1), time_offset, slice_start, slice_end)
                modality_vals.extend([ModalityType.text.value] * keep_len)
                text_chunks.append(it["tokens"].view(-1)[slice_start:slice_end])
                time_offset += seg_len
            else:
                dims = tuple(int(x) for x in it["dims"])
                append_coords(dims, time_offset, slice_start, slice_end)
                modality_vals.extend([ModalityType.image.value] * keep_len)

                vision_chunks.append(it["patches"])
                rd = it["real_dims"]
                token_grids.append((int(rd[1]), int(rd[2])))
                token_offsets.append(slice_start)
                token_lengths.append(keep_len)

                time_offset += dims[0]

            cursor = seg_end

        # ---- choose device
        base_device = (
            vision_chunks[0].device
            if vision_chunks
            else (text_chunks[0].device if text_chunks else torch.device("cpu"))
        )

        # ---- finalize tensors
        modality_tensor = (
            torch.tensor(modality_vals, dtype=torch.long, device=base_device).unsqueeze(0)
            if modality_vals
            else torch.zeros((1, 0), dtype=torch.long, device=base_device)
        )
        position_ids = (
            torch.tensor(coords, dtype=torch.long, device=base_device).reshape(1, len(coords), 3)
            if coords
            else torch.zeros((1, 0, 3), dtype=torch.long, device=base_device)
        )

        text_concat = torch.cat(text_chunks, dim=0).to(device=base_device, dtype=torch.long) if text_chunks else None
        text_token_ids = (
            text_concat.unsqueeze(0)
            if text_concat is not None
            else torch.zeros((1, 0), dtype=torch.long, device=base_device)
        )

        if vision_chunks:
            vision_patches = torch.cat(vision_chunks, dim=0)
            dev = vision_patches.device
            vision_token_grids = torch.tensor(token_grids, dtype=torch.long, device=dev)
            vision_token_offsets = torch.tensor(token_offsets, dtype=torch.long, device=dev)
            vision_token_lengths = torch.tensor(token_lengths, dtype=torch.long, device=dev)
        else:
            vision_patches = vision_token_grids = vision_token_offsets = vision_token_lengths = None

        return {
            "vision_patches": vision_patches,
            "vision_token_grids": vision_token_grids,
            "vision_token_offsets": vision_token_offsets,
            "vision_token_lengths": vision_token_lengths,
            "modality_tensor": modality_tensor,
            "position_ids": position_ids,
            "text_token_ids": text_token_ids,
        }

    def __call__(
        self,
        text: Union[str, list[str]],
        images: Optional[Union["Image", list["Image"]]] = None,
        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,
        **kwargs,
    ) -> "BatchFeature":
        texts = [text] if isinstance(text, str) else text
        if len(texts) != 1:
            raise ValueError("IsaacProcessor currently supports batch_size=1")

        images_list = None
        if images is not None:
            images_list = [images] if isinstance(images, Image) else images
            n_tok = texts[0].count(self.vision_token)
            if n_tok != len(images_list):
                raise ValueError(
                    f"Number of {self.vision_token} tokens in text ({n_tok}) must match number of images ({len(images_list)})"
                )

        packed = self._pack_single(texts[0], images_list)

        fill_value = getattr(self.tokenizer, "pad_token_id", 0)
        if fill_value is None or fill_value < 0:
            fill_value = 0

        tokens = tensor_stream_token_view(packed["text_token_ids"], packed["modality_tensor"], fill_value=fill_value)
        input_ids = (
            torch.as_tensor(tokens, dtype=torch.long) if return_tensors in (TensorType.PYTORCH, "pt") else tokens
        )

        return BatchFeature(data={"input_ids": input_ids, "packed_inputs": packed})


__all__ = ["IsaacProcessor"]
