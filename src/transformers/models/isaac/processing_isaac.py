#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/isaac/modular_isaac.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_isaac.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 Perceptron, Inc and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import itertools
import math
import re
from collections.abc import Callable, Iterable
from typing import Any, Optional, Union

from ...feature_extraction_utils import BatchFeature
from ...models.auto.tokenization_auto import AutoTokenizer
from ...processing_utils import ProcessorMixin
from ...utils import TensorType
from ...utils.import_utils import is_torch_available, is_vision_available
from .configuration_isaac import IsaacConfig
from .modeling_isaac import Event, ModalityType, Stream, TensorStream


if is_torch_available():
    import torch


if is_vision_available():
    from PIL.Image import Image
else:
    Image = None


def _infer_device_from_streams(streams: list[Stream]) -> torch.device | str:
    return next(
        (ev.data.device for stream in streams for ev in stream.events if isinstance(ev.data, torch.Tensor)),
        "cpu",
    )


def _compact_stream_events(stream: Stream) -> torch.Tensor:
    assert all([(isinstance(ev.data, torch.Tensor) and ev.is_measured) for ev in stream.events]), (
        "stream_apply(compact=True) only works for streams with events that have measured tensor data"
    )
    return torch.cat([ev.data for ev in stream.events]).contiguous()


def _map_stream(
    stream: Stream,
    map_fn: Callable[[Event], Any] | None,
    *,
    copy_unchanged: bool,
) -> tuple[str, Stream | list[Any]]:
    if map_fn is None:
        return "events", stream if not copy_unchanged else stream.shallow_copy()

    mapped_events: list[Event] = []
    flat_values: list[Any] = []
    mode: str | None = None

    for ev in stream:
        out = map_fn(ev)
        if out is None:
            out = {}

        if isinstance(out, dict):
            if mode is None:
                mode = "events"
            elif mode != "events":
                raise ValueError("map_fn must consistently return a dict or an iterable for every event")

            if not out:
                mapped_events.append(ev if not copy_unchanged else ev.shallow_copy())
                continue

            new_ev = ev.shallow_copy()
            for k, v in out.items():
                setattr(new_ev, k, v)
            mapped_events.append(new_ev)
            continue

        if isinstance(out, Iterable) and not isinstance(out, (str, bytes)):
            if mode is None:
                mode = "flat"
            elif mode != "flat":
                raise ValueError("map_fn must consistently return a dict or an iterable for every event")

            flat_values.extend(out)
            continue

        raise TypeError(f"map_fn must return a dict or iterable, got {type(out)}")

    if mode is None:
        mode = "events"
        mapped_events = stream.events if not copy_unchanged else [ev.shallow_copy() for ev in stream.events]
        return mode, Stream(mapped_events, priority=stream.priority)

    if mode == "events":
        return mode, Stream(mapped_events, priority=stream.priority)

    return mode, flat_values


def stream_apply(
    stream_like: Stream | TensorStream,
    map_fn: Callable[[Event], Any] | None = None,
    *,
    compact: bool = False,
    copy_unchanged: bool = False,
) -> Stream | TensorStream | torch.Tensor | list[Any] | list[list[Any]]:
    """
    Unified helper to (optionally) map over events and (optionally) compact to tensors.

    Args:
        stream_like: A ``Stream`` or ``TensorStream`` instance.
        map_fn: Optional callable applied to every ``Event``. It may return a dict of field deltas
            (structural map) or an iterable of values (token map). Returning ``None`` is treated as ``{}``.
        compact: When True, returns a tensor (``(B, T)`` for ``TensorStream``, ``(T,)`` for ``Stream``).
        copy_unchanged: When True, shallow-copy events even if unchanged.

    Returns:
        Stream/TensorStream, torch.Tensor, or list(s) depending on ``compact`` and ``map_fn`` mode.
    """
    is_batch = isinstance(stream_like, TensorStream)

    streams = stream_like.streams if is_batch else [stream_like]
    mapped_streams: list[Stream] = []
    flat_batches: list[list[Any]] = []
    mode: str | None = None

    for stream in streams:
        stream_mode, payload = _map_stream(stream, map_fn, copy_unchanged=copy_unchanged)
        if mode is None:
            mode = stream_mode
        elif mode != stream_mode:
            raise ValueError("map_fn must return the same type (dict vs iterable) for every stream")

        if stream_mode == "events":
            mapped_streams.append(payload)  # type: ignore[arg-type]
        else:
            flat_batches.append(payload)  # type: ignore[arg-type]

    if mode is None:
        mode = "events"
        mapped_streams = streams if not copy_unchanged else [s.shallow_copy() for s in streams]

    if not compact:
        if mode == "events":
            return TensorStream(mapped_streams) if is_batch else mapped_streams[0]
        return flat_batches if is_batch else flat_batches[0]

    if mode == "events":
        compacted = [_compact_stream_events(s) for s in mapped_streams]
        return torch.stack(compacted).contiguous() if is_batch else compacted[0]

    device = _infer_device_from_streams(streams)
    flat_values: list[Any] = list(itertools.chain.from_iterable(flat_batches)) if is_batch else flat_batches[0]
    tensor = torch.tensor(flat_values, dtype=torch.long, device=device)
    if is_batch:
        B, T = stream_like.shape  # type: ignore[union-attr]
        tensor = tensor.reshape(B, T)
    return tensor.contiguous()


def compute_mrope_pos_tensor(ts: TensorStream, n_pos_dims: int = 3) -> torch.Tensor:
    """
    Create a (batch, T, n_pos_dims) position tensor in one sweep.
    The first dim is the running â€œtimeâ€ index, the rest are spatial (or 1-fillers).

    Args:
        ts         : TensorStream
        n_pos_dims : total coordinate dimensions (default 3)

    Returns:
        torch.LongTensor  - shape (batch_size, seq_len, n_pos_dims)
    """

    # Manually iterate through streams and events like map_compact does,
    # but maintain cumulative time offset for each stream
    all_coords = []
    for stream in ts.streams:  # one Stream == one batch sample
        cumulative_offset = 0  # running time index for this stream

        for event in stream:
            # --- build coordinate grid for THIS event using itertools (no tensor ops) ---
            dims = (event.dims() or [1]) + [1] * (n_pos_dims - len(event.dims() or []))

            # Create ranges for each dimension (similar to old _finalize implementation)
            first_dim = range(cumulative_offset, cumulative_offset + dims[0])
            cumulative_offset += dims[0]  # advance time for the next event
            other_dims = [range(d) for d in dims[1:]]

            # Use itertools.product to create all coordinate combinations
            full_coords = list(itertools.product(first_dim, *other_dims))

            # Slice if the event is partial
            s, e = event.idx_range
            coords = full_coords[s:e]

            # Extend the flattened coordinate list
            all_coords.extend(coords)

    # Convert to tensor and reshape to (B, T, n_pos_dims)
    B, T = ts.shape
    return torch.tensor(all_coords, dtype=torch.long, device=ts.device).reshape(B, T, n_pos_dims)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Generic event-labelling helper
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def event_mask(
    ts: TensorStream,
    tag_fn: Callable[[Event], int | None],
    default: int = -1,
) -> torch.Tensor:
    """
    Build a (batch, seq_len) LongTensor whose value for every *token*
    is given by `tag_fn(event)`, falling back to `default` when the
    function returns None.

    The work is done in a single pass via `map  â†’  compact`.
    """

    def to_label(ev: Event) -> Any:
        label = tag_fn(ev)
        if label is None:
            label = default
        return [label] * ev.num_tokens()

    return stream_apply(ts, to_label, compact=True).squeeze(-1)


def modality_mask(ts: TensorStream) -> torch.Tensor:
    return event_mask(ts, lambda ev: ev.type.value)


def tensor_stream_token_view(ts: TensorStream) -> torch.Tensor:
    """
    Return a (B, T) token view by summing across the last dim of every
    event and flattening over the selected token range.
    """

    def to_token_view(ev: Event) -> list[int]:
        # collapse all but the last dim, cast to long
        flat = ev.data.sum(dim=-1).long().reshape(-1)
        if ev.idx_range is not None:
            s, e = ev.idx_range
            return flat[s:e].tolist()
        else:
            return flat.tolist()

    return stream_apply(ts, to_token_view, compact=True)  # shape (B, T)


def tensor_stream_to_packed_inputs(tensor_stream: TensorStream) -> dict[str, Optional[torch.Tensor]]:
    """
    Extract plain tensor payloads from a TensorStream so downstream code can start
    bypassing the Event/Stream abstraction. The returned tensors cover the Event
    fields we previously relied on:

    - modality_tensor mirrors `Event.type` for every token.
    - position_ids materialize MRoPE-ready coordinates from `Event.idx_range` and `Event.dims`.
    - vision_token_grids store the real spatial grid from `Event.dims(real)`.
    - vision_token_offsets/vision_token_lengths mirror `Event.idx_range` for vision events after truncation.
    - vision_patches keeps `Event.data` (real patch vectors) concatenated.
    """

    vision_events = [ev for stream in tensor_stream.streams for ev in stream if ev.type == ModalityType.image]

    seq_patches: Optional[torch.Tensor]
    token_grids: Optional[torch.Tensor]
    token_offsets: Optional[torch.Tensor]
    token_lengths: Optional[torch.Tensor]

    if vision_events:
        patch_chunks: list[torch.Tensor] = []
        grid_rows: list[tuple[int, int]] = []
        offset_rows: list[int] = []
        length_rows: list[int] = []

        for ev in vision_events:
            event_data = ev.data
            if event_data.dim() > 2:
                event_data = event_data.view(event_data.size(0), -1)

            # idx_range describes how much of this event survives truncation
            token_span = ev.idx_range
            if token_span is None:
                token_span = (0, math.prod(ev.dims() or [event_data.shape[0]]))
            start, end = token_span

            patch_chunks.append(event_data)
            dims_real = ev.dims(False) or ev.dims() or [event_data.shape[0], 1, 1]
            if len(dims_real) < 3:
                dims_real = list(dims_real) + [1] * (3 - len(dims_real))
            grid_rows.append((int(dims_real[1]), int(dims_real[2])))
            offset_rows.append(int(max(0, start)))
            length_rows.append(int(max(0, end - start)))

        seq_patches = torch.cat(patch_chunks, dim=0)
        token_grids = torch.tensor(grid_rows, dtype=torch.long, device=seq_patches.device)
        token_offsets = torch.tensor(offset_rows, dtype=torch.long, device=seq_patches.device)
        token_lengths = torch.tensor(length_rows, dtype=torch.long, device=seq_patches.device)
    else:
        seq_patches = None
        token_grids = None
        token_offsets = None
        token_lengths = None

    return {
        "vision_patches": seq_patches,
        "vision_token_grids": token_grids,
        "vision_token_offsets": token_offsets,
        "vision_token_lengths": token_lengths,
        "modality_tensor": modality_mask(tensor_stream),
        "position_ids": compute_mrope_pos_tensor(tensor_stream),
    }


# ============================================================================
# Processor Components
# ============================================================================


def create_text_event(tokenizer: AutoTokenizer, text: str, time: float = 0.0) -> "Event":
    r"""Wrap a text into an `Event` compatible with the multimodal TensorStream.

    Args:
        tokenizer (`AutoTokenizer`):
            Tokenizer used to convert text into model vocabulary ids.
        text (`str`):
            Plain-text fragment to encode.
        time (`float`, *optional*, defaults to 0.0):
            Timeline coordinate associated with the event. Both start and end times use the same value because text
            segments are instantaneous in the scheduler.

    Returns:
        `Event`: Event carrying a `(num_tokens, 1)` tensor of token ids with matching
        metadata so that downstream processors can compute modality-specific embeddings.
    """
    tokens = tokenizer.encode(text, add_special_tokens=False, return_tensors="pt").squeeze(0)

    # Calculate dimensions for the event
    num_tokens = len(tokens)
    dims_virtual = [num_tokens, 1]  # [sequence_length, 1]
    dims_real = dims_virtual.copy()

    # Ensure tokens has the right shape for tensor_stream_token_view
    # It expects a 2D tensor where sum(dim=-1) gives the token IDs
    if tokens.dim() == 1:
        tokens = tokens.unsqueeze(-1)

    return Event(
        data=tokens,
        type=ModalityType.text,
        time=(time, time),
        dims_virtual=dims_virtual,
        dims_real=dims_real,
        idx_range=(0, num_tokens),
    )


# ============================================================================
# Processor
# ============================================================================


class IsaacProcessor(ProcessorMixin):
    attributes = ["image_processor", "tokenizer"]
    image_processor_class = ("IsaacImageProcessorFast",)
    tokenizer_class = ("Qwen2Tokenizer",)

    def __init__(
        self,
        image_processor,
        tokenizer,
        *,
        vision_token: str = "<image>",
        max_sequence_length: int = 16384,
        rescale_factor: Optional[float] = None,
        config: Optional[Union[IsaacConfig, dict]] = None,
    ) -> None:
        if tokenizer is None:
            raise ValueError("`tokenizer` must be provided to initialize IsaacProcessor.")

        if isinstance(config, dict):
            config = IsaacConfig(**config)

        if config is not None:
            max_sequence_length = config.max_sequence_length
            vision_token = config.vision_token
            rescale_factor = config.vision_rescale_factor

        resolved_rescale_factor = float(rescale_factor) if rescale_factor is not None else float(1 / 255)

        if config is not None:
            config.vision_rescale_factor = resolved_rescale_factor

        self.image_processor = image_processor

        super().__init__(image_processor, tokenizer)
        self.current_processor = self.image_processor
        self.config = config

        # Mirror tokenizer chat template so ProcessorMixin.apply_chat_template works.
        self.chat_template = getattr(self.tokenizer, "chat_template", None)

        self.vision_token = vision_token
        self.max_sequence_length = max_sequence_length

    def build_event_stream_simple(
        self,
        text: str,
        images: Optional[list[Image]] = None,
    ) -> "Stream":
        events = []
        # Process text and images
        # Find all occurrences of vision token

        pattern = re.escape(self.vision_token)
        parts = re.split(f"({pattern})", text)  # Keep the delimiter in the result

        image_idx = 0
        for current_time, part in enumerate(parts):
            if part == self.vision_token:
                # Replace vision token with image event
                if images is None or image_idx >= len(images):
                    raise ValueError("Encountered vision token without a corresponding image.")

                features = self.image_processor(
                    images=images[image_idx],
                    return_tensors=TensorType.PYTORCH,
                )

                patches = features["patches"][0]  # (H_tokens, W_tokens, embed)
                virtual_dims = features["virtual_pixel_size"][0].tolist()
                real_dims = features["real_pixel_size"][0].tolist()

                vision_event = Event(
                    data=patches.reshape(-1, patches.shape[-1]),
                    type=ModalityType.image,
                    time=(current_time, current_time),
                    dims_virtual=virtual_dims,
                    dims_real=real_dims,
                    idx_range=(0, math.prod(virtual_dims)),
                )
                events.append(vision_event)
                image_idx += 1
            elif part:  # Non-empty text part
                # tokens = self.text_processor.tokenize(part, add_special_tokens=False)
                text_event = create_text_event(self.tokenizer, part, time=current_time)
                events.append(text_event)

        # Create stream without scheduling (events already in order)
        return Stream(events, priority=[ModalityType.text, ModalityType.image])

    def __call__(
        self,
        text: Union[str, list[str]],
        images: Optional[Union[Image, list[Image]]] = None,
        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,
        return_tensor_stream: bool = True,
        **kwargs,
    ) -> BatchFeature:
        """
        Process text and images into packed tensors (and optionally TensorStream for compatibility).
        Args:
            text: Input text or list of texts with vision tokens
            images: PIL image or list of images (optional)
            return_tensors: Format for output tensors
            return_tensor_stream: When True, include the legacy TensorStream in the output.

        Returns:
            BatchFeature with input_ids, packed_inputs, and optionally tensor_stream. packed_inputs bundles the
            plain tensors (vision_patches, vision_token_grids, modality_tensor, position_ids, vision_token_offsets,
            vision_token_lengths) needed to describe the multimodal sequence without TensorStream.
        """
        # Normalize inputs to lists
        if isinstance(text, str):
            texts = [text]
        else:
            texts = text

        if images is not None:
            if isinstance(images, Image):
                images_list = [images]
            else:
                images_list = images
        else:
            images_list = None

        if len(texts) != 1:
            raise ValueError("IsaacProcessor currently supports batch_size=1")
        if images_list is not None:
            # Count vision tokens in text to validate image count
            vision_token_count = texts[0].count(self.vision_token)
            if vision_token_count != len(images_list):
                raise ValueError(
                    f"Number of {self.vision_token} tokens in text ({vision_token_count}) "
                    f"must match number of images ({len(images_list)})"
                )

        # Build event stream
        stream = self.build_event_stream_simple(
            text=texts[0],
            images=images_list,
        )

        # Create TensorStream
        tensor_stream = TensorStream([stream])

        # Slice to max length if needed
        _, T = tensor_stream.shape
        if T > self.max_sequence_length:
            tensor_stream = ts_slice(tensor_stream, start=T - self.max_sequence_length, end=T)

        # Get token view
        tokens = tensor_stream_token_view(tensor_stream)
        if return_tensors in (TensorType.PYTORCH, "pt"):
            input_ids = torch.as_tensor(tokens, dtype=torch.long)
        else:
            input_ids = tokens

        packed_inputs = tensor_stream_to_packed_inputs(tensor_stream)
        data = {
            "input_ids": input_ids,
            "packed_inputs": packed_inputs,
        }
        if return_tensor_stream:
            data["tensor_stream"] = tensor_stream

        return BatchFeature(data=data)


__all__ = ["IsaacProcessor"]
